[{"content":"Run a nodejs program as node --tls-keylog=/tmp/tlskey index.js where the tls key will be stored on that path and add this tlskey path to wire shark to decrypt the msg by doing following\nGo to Preferences-\u0026gt;Protocols-\u0026gt;TLS and edit the path as shown in the screenshot below. ![[Pasted image 20240212083026.png]]\nMongodb uses Mongodb wire protocol at application level to know more check here\n","permalink":"https://programmerraja.github.io/blog/post/2024/how-to-wireshark-the-mongodb/","summary":"Run a nodejs program as node --tls-keylog=/tmp/tlskey index.js where the tls key will be stored on that path and add this tlskey path to wire shark to decrypt the msg by doing following\nGo to Preferences-\u0026gt;Protocols-\u0026gt;TLS and edit the path as shown in the screenshot below. ![[Pasted image 20240212083026.png]]\nMongodb uses Mongodb wire protocol at application level to know more check here","title":"How to wireshark the mongodb"},{"content":"to rember the design pattern narrate it as real life story\nresources https://youtu.be/taj_inLi-pY?si=2qu_7nqx6YRwX0Co\nCrational Design pattern Once upon a time in the colorful land of Toyland, there were magical creatures known as Creators. Each Creator had a unique way of bringing toys to life, showcasing various creational design patterns.\nAbstract Factory Pattern: The Artisan Workshop\nIn the heart of Toyland, there was an Artisan Workshop where the master toymaker, the Abstract Artisan, crafted a variety of toys. Depending on the season – be it spring, summer, fall, or winter – the Artisan would use a specialized machine to create toys that matched the spirit of that season. The workshop could seamlessly switch between crafting delightful flower-shaped puzzles in spring to building snowflake-adorned snow globes in winter.\nDesign Pattern Explanation: The Abstract Factory pattern is like the Artisan Workshop, creating toys that match the theme of each season using a specialized machine.\nclass SeasonalToyFactory { createToy() { throw new Error(\u0026#34;Abstract method. Please implement in concrete subclass.\u0026#34;); } } class SpringToyFactory extends SeasonalToyFactory { createToy() { return new FlowerToy(); } } class WinterToyFactory extends SeasonalToyFactory { createToy() { return new SnowmanToy(); } } Builder Pattern: The Custom Toy Shop\nRight next to the Artisan Workshop, there was a Custom Toy Shop run by a jolly toymaker, the Builder Elf. Children from Toyland could visit the shop and choose the parts they wanted for their custom toy – selecting the type of body, color, and accessories. The Builder Elf would then assemble the chosen parts, creating a unique toy tailored to each child\u0026rsquo;s preferences.\nDesign Pattern Explanation: The Builder pattern is like the Custom Toy Shop, where a toymaker assembles a toy based on the specific choices made by the children.\nclass CustomToyBuilder { constructor() { this.toy = new Toy(); } buildBody(body) { this.toy.setBody(body); } buildColor(color) { this.toy.setColor(color); } buildAccessories(accessories) { this.toy.setAccessories(accessories); } getToy() { return this.toy; } } Factory Method Pattern: The Animal Wonderland Factory\nA bit further away, there was an Animal Wonderland Factory. Here, the Animal Creator designed a variety of stuffed animals, each with its own special feature. The factory had different machines, like the Bear-Maker and Bunny-Maker, each specializing in creating a particular type of stuffed animal. Every day, the Animal Creator would decide which adorable animal to make and use the corresponding machine.\nDesign Pattern Explanation: The Factory Method pattern is like the Animal Wonderland Factory, where different machines create various stuffed animals based on the decision of the Animal Creator.\nclass AnimalCreator { createAnimal() { throw new Error(\u0026#34;Abstract method. Please implement in concrete subclass.\u0026#34;); } } class BearCreator extends AnimalCreator { createAnimal() { return new Bear(); } } class BunnyCreator extends AnimalCreator { createAnimal() { return new Bunny(); } } Singleton Pattern: The Timeless Toy Museum\nIn the center of Toyland stood the Timeless Toy Museum, housing the most beloved and classic toys. The museum had a magical guardian, the Singleton Guardian, who ensured there was only one entrance to the museum. No matter how many children wanted to enter, they all had to go through the same door, preserving the uniqueness and timelessness of the exhibited toys.\nDesign Pattern Explanation: The Singleton pattern is like the Timeless Toy Museum, where the Singleton Guardian ensures there\u0026rsquo;s only one entrance, maintaining the uniqueness and timelessness of the exhibited toys.\nclass SingletonGuardian { static #instance; constructor() { if (!SingletonGuardian.#instance) { SingletonGuardian.#instance = this; } return SingletonGuardian.#instance; } } Prototype Pattern: The Replicator\u0026rsquo;s Workshop\nNestled on the outskirts of Toyland, there was the Replicator\u0026rsquo;s Workshop. Here, the Prototype Puppeteer, a wizard with a magical wand, had the ability to duplicate toys effortlessly. When a special toy was created, the Puppeteer would wave their wand, and presto – an exact replica would appear! This allowed for the mass production of popular toys without starting from scratch each time.\nDesign Pattern Explanation: The Prototype pattern is like the Replicator\u0026rsquo;s Workshop, where the Prototype Puppeteer magically duplicates toys, making exact copies without crafting each one from the beginning.\nclass Toy { clone() { return Object.assign(Object.create(Object.getPrototypeOf(this)), this); } } class PrototypePuppeteer { makeDuplicate(originalToy) { return originalToy.clone(); } } Structural Design Patterns: In the charming town of Toyland, where toys came to life, there was an ingenious architect named StructureBuilder. StructureBuilder loved designing magical playhouses using various structural design patterns, each contributing to the uniqueness and durability of the toy structures.\nAdapter Pattern: The Magical Toy Connector\nOne day, the StructureBuilder wanted to connect a set of new magnetic building blocks with the existing wooden ones. To make them work together seamlessly, the architect created a Magical Toy Connector. This connector translated the language of magnets into the language of wooden pegs, allowing different types of building blocks to connect effortlessly.\nclass WoodenBlock { insertIntoSlot() { // Insert into a wooden slot } } class MagneticBlock { attachWithMagnet() { // Attach with a magnet } } class MagicalToyConnector { connectWoodenToMagnetic(woodenBlock, magneticBlock) { woodenBlock.insertIntoSlot(); magneticBlock.attachWithMagnet(); } } Bridge Pattern: The Enchanted Rope Bridge\nStructureBuilder wanted to create an enchanting rope bridge connecting two treehouses. Using the Bridge pattern, the architect separated the abstraction of the bridge from its implementation. Now, whether it was a magical vine or a sparkling thread, it could be easily swapped without affecting the overall structure.\nclass RopeBridge { constructor(implementation) { this.implementation = implementation; } crossBridge() { this.implementation.cross(); } } class MagicalVineBridge { cross() { // Cross the bridge using magical vines } } class SparklingThreadBridge { cross() { // Cross the bridge using sparkling threads } Composite Pattern: The Tower of Imagination\nThe StructureBuilder dreamt of creating a Tower of Imagination using a variety of building blocks. With the Composite pattern, individual blocks and complex structures could be treated uniformly. The Tower of Imagination could have both simple blocks and other towers as its components, forming a magnificent and unified structure.\nclass BuildingBlock { build() { // Build the block } } class TowerOfImagination { constructor() { this.components = []; } addComponent(component) { this.components.push(component); } build() { for (const component of this.components) { component.build(); } } } Decorator Pattern: The Colorful Paintbrush\nStructureBuilder wanted to add vibrant colors to a plain wooden carousel. Instead of modifying the carousel directly, the architect used the Decorator pattern. A ColorDecorator wrapped around the carousel, adding a burst of colors without altering its underlying structure.\nclass Carousel { spin() { // Spin the carousel } } class ColorDecorator { constructor(carousel, color) { this.carousel = carousel; this.color = color; } spin() { this.carousel.spin(); this.addColor(); } addColor() { // Add the chosen color to the carousel } } Proxy Pattern: The Guardian Toy Sentinel\nToyland had a cherished collection of rare and delicate toys housed in the Precious Toy Vault. To safeguard these treasures, StructureBuilder employed the Proxy pattern. The Guardian Toy Sentinel acted as a protective proxy, ensuring that only those with special permissions could access the precious toys.\nclass PreciousToyVault { open() { // Open the vault } } class GuardianToySentinel { constructor(vault) { this.vault = vault; this.accessGranted = false; } grantAccess() { // Check permissions and grant access this.accessGranted = true; } openVault() { if (this.accessGranted) { this.vault.open(); } else { console.log(\u0026#34;Access denied. Seek permission from the Guardian Toy Sentinel.\u0026#34;); } } } Facade Pattern: The Magic Toy Workshop\nIn the heart of Toyland, StructureBuilder established the Magic Toy Workshop, a place of enchantment where various toys were crafted. To simplify the complexity of the toy-making process, StructureBuilder implemented the Facade pattern. The Magic Toy Workshop acted as a facade, providing a single entry point for creating different types of toys.\nclass MagicToyWorkshop { createTeddyBear() { // Create a teddy bear } createDoll() { // Create a doll } createToyCar() { // Create a toy car } } Flyweight Pattern: The Whirling Windmill Factory\nIn the playful outskirts of Toyland, there stood the Whirling Windmill Factory, a place where windmills of various shapes and colors were crafted. StructureBuilder, mindful of efficiency, implemented the Flyweight pattern. The shared intrinsic properties of the windmills were stored externally, allowing the factory to create a multitude of windmills without duplicating common features.\nclass WhirlingWindmill { constructor(color, shape) { this.color = color; this.shape = shape; } whirl() { // Whirl the windmill } } class WindmillFactory { constructor() { this.windmills = {}; } createWindmill(color, shape) { const key = `${color}_${shape}`; if (!this.windmills[key]) { this.windmills[key] = new WhirlingWindmill(color, shape); } return this.windmills[key]; } } ","permalink":"https://programmerraja.github.io/blog/notes/2024/design-pattern/","summary":"to rember the design pattern narrate it as real life story\nresources https://youtu.be/taj_inLi-pY?si=2qu_7nqx6YRwX0Co\nCrational Design pattern Once upon a time in the colorful land of Toyland, there were magical creatures known as Creators. Each Creator had a unique way of bringing toys to life, showcasing various creational design patterns.\nAbstract Factory Pattern: The Artisan Workshop\nIn the heart of Toyland, there was an Artisan Workshop where the master toymaker, the Abstract Artisan, crafted a variety of toys.","title":"Design pattern"},{"content":"Book Core concepts It explains principles and concepts I believe a senior architect must understand deeply and discusses how to employ those principles to manage uncertainty\nBooks for leadership The Hard Things About Hard Things by Ben Horowitz, Trillion Dollar Coach by Eric Schmidt et al., Team of Teams: New Rules of Engagement for a Complex World by Stanley McChrystal Good Strategy, Bad Strategy by Richard Rumel Three layers of architecture of The Open Group Architecture Framework (TOGAF)\nTwo prominent approaches to system architecture:\nWaterfall -\u0026gt;identify the system’s requirements in full detail beforehand and start building Agile -\u0026gt; Iterative way (collaborating with users to refine requirements and construct a system that can genuinely benefit the user) Understanding Systems, Design, and Architecture When writing a cloud app, we have two choices: We can choose a single cloud, taking advantage of its unique strengths for our application, or we can make the application portable across several cloud providers\nHow to Design a System Five questions and 7 Principles to understand the context of system we building\nWhen is the best time to market? If the feature need to go urgent becasue of market urgent. we can design the way simple and fast. and we also ready to re-write it. What is the skill level of the team? What is our system’s performance sensitivity? When can we rewrite the system? What are the hard problems? Seven principles\nDrive everything from the user’s journey. Use an iterative thin slice strategy Unless you have a specific reason, always start with simple architectural choices. Measure the system, find the bottlenecks, and improve the system later. On each iteration, add the most value for the least effort to support more users Make decisions and absorb the risks Design deeply things that are hard to change but implement them slowly Eliminate the unknowns and learn from the evidence by working on hard problems early and in parallel Understand the trade-offs between cohesion and flexibility in the software architecture Mental Models for Understanding and Explaining System Performance Eight mental models that help us think about and understand performance\nCost of Switching to the Kernel Mode from the User Mode Every time anapplication enters kernel mode, a context switch occurs, which adds nonessential costs to the system, such as time to save the stack and to rest the cache. To improve performance, we need to reduce the number of system calls. Operations Hierarchy Operation Time/Speed (Example) Description L1 Cache Reference 0.5 ns Accessing data from Level 1 cache in the CPU. L2 Cache Reference 7 ns Accessing data from Level 2 cache in the CPU. L3 Cache Reference 20 ns Accessing data from Level 3 cache in the CPU. Main Memory (RAM) Access 100 ns Retrieving data from the system\u0026rsquo;s RAM. SSD Storage Access 1,000,000 ns (1 ms) Accessing data from Solid State Drive (SSD). HDD Storage Access 10,000,000 ns (10 ms) Accessing data from Hard Disk Drive (HDD). Network Packet Round Trip (US to India) 150,000,000 ns (150 ms) Sending a packet from the US to India and receiving the acknowledgment. CPU Processing (Instruction) 0.2 ns per instruction Executing a single CPU instruction. GPU Processing (Parallel) 50 ns per parallel task Executing parallel tasks on a Graphics Processing Unit (GPU). PCIe Data Transfer (Device to CPU) 2,000 ns (2 us) Transferring data between a peripheral device and the CPU via PCIe. Context Switch (Kernel) 1,000 ns (1 us) Switching between different processes in the kernel. RAM-to-Cache Transfer 20 ns Copying data from RAM to cache in the CPU. Database Query (Local) 10,000,000 ns (10 ms) Executing a database query on a local server. Database Query (Remote) 50,000,000 ns (50 ms) Executing a database query on a remote server. System Call (Linux) 1,000 ns (1 us) Initiating a system call in a Linux environment. I/O Operation (Disk Write) 10,000 ns (10 us) Writing data to a disk. I/O Operation (Network Send) 1,000,000 ns (1 ms) Sending data over a network. I/O Operation (Network Receive) 1,000,000 ns (1 ms) Receiving data over a network. Context Switching Overhead Switching processes adds an overhead cost of about 5–7 microseconds Amdahl’s Law is used to predict speed up of a task execution time when it’s scaled to run on multiple processors. It simply states that the maximum speed up will be limited by the serial fraction of the task execution as it will create resource contention. one woman nine months to make one baby, “nine women can’t make a baby in one month Assume if have 3 thread that doing a single task it take 2 sec but the most of the time will spend on maintining the 3 thread and resources sharing Parellel process are efficient when they are independ Universal Scalability Law says that actual speedup is even worse than Amdahl’s law due to shared variables. USL defines a new parameter coherency, which is the overhead added by communication between multiple processes, threads, or nodes. Latency and Utilization Trade-offs Only a single thread can use the most resources at a given time, which forces threads to wait and take turns Designing for Throughput with the Maximal Useful Utilization (MUU) Model Adding Latency Limits Optimization Techniques To optimize, we need to decide where the bottlenecks are. Bottlenecks usually come in one of three forms:\nOne of the resources (e.g., CPU, I/O, or memory) is the bottleneck. Thread models are causing critical resources to be idle. Resources are wasted on nonessential tasks (e.g., context switches, GC). CPU Optimization Techniques Optimize Individual Tasks Optimize Memory Maximize CPU Utilization I/O Optimization Techniques Avoid I/O (use a cache) Buffering Send Early, Receive Late, Don’t Ask but Tell Prefetching Append-Only Processing (Kafka) Memory Optimization Techniques Too Many Cache Misses Latency Optimization Techniques Do Work in Parallel Reduce I/O CPU Utilization is Wrong CPU utilization is a metric that measures the percentage of time the CPU spends executing non-idle tasks. It doesn\u0026rsquo;t necessarily mean the CPU is busy with computation; it\u0026rsquo;s more about the time the CPU is not running the idle thread. The idle thread is a special task that runs when the CPU has no other tasks to perform. The operating system kernel tracks CPU utilization during context switches. So high %CPU to mean that the processing unit is the bottleneck, which is wrong because CPU is capable of doing the process it may wait for I/O or something . Source: https://opensource.com/article/18/4/cpu-utilization-wrong The USE Method The Utilization Saturation and Errors (USE) Method is a methodology for analyzing the performance of any system\nFor every resource, check utilization, saturation, and errors.\nresource: all physical server functional components (CPUs, disks, busses, \u0026hellip;) utilization: the average time that the resource was busy servicing work saturation: the degree to which the resource has extra work which it can\u0026rsquo;t service, often queued errors: the count of error events Refer https://fs.blog/mental-models/ http://highscalability.com/blog/2012/5/16/big-list-of-20-common-bottlenecks.html https://opensource.com/article/18/4/cpu-utilization-wrong https://en.wikipedia.org/wiki/Queueing_theory https://www.brendangregg.com/usemethod.html [bottleneck] https://medium.freecodecamp.org/what-makes-apache-kafka-so-fast-a8d4f94ab145 An Analysis of Web Servers Architectures Performances on Commodity Multicore https://hal.inria.fr/hal-00674475/document https://ocw.mit.edu/courses/6-851-advanced-data-structures-spring-2012/video_galleries/lecture-videos/ Understanding User Experience (UX) Few concepts or principles that help us to design a good UX.\nUnderstand the Users Do as Little as Possible Good Products Do Not Need a Manual: Its Use Is Self-Evident Think in Terms of Information Exchange Users come to our system to get something done. The faster they can find what they need to do and do it, the happier they will be. If we can provide that UX without asking them anything, that’s even better. Make Simple Things Simple Design UX Before Implementation Note: having UX expertise on the team is a must\nMacro Architecture Spliting for serivce is macro Architecture.\nMacro Architectural Building Blocks are\nData Management (DB,) Routers and Messaging (API Gateway,loadbalancer,message broker) Executors (Actual server) Security Communication (Distributed hash tables,Gossip architectures,Tree of responsibility patterns) Macro Architecture: Coordination Drive flow from the client Call All API calls to service from client\nAnother service Have seprate service which will cordinate with other and return the result.\nImplement Choreography Event driven system, where each participant in the process listens to different events and carries out their individual part. Each action generates asynchronous events, which trigger participants downstream\nRefer\nhttps://www.thoughtworks.com/insights/blog/scaling-microservices-event-stream Macro Architecture: Preserving Consistency of State Two-phase commit protocol Approaches to going beyond transactions Redefining the Problem to Require Lesser Guarantees\nFigure out a way to resolve complex situations. Provide a button for users to forcefully refresh the page if they can tell that it is outdated Using Compensations\nStarbucks Does Not Use Two-Phase Commit The key idea is that if an action fails, you can compensate. Use compenstations if below 3 have statisfy Each individual operation can be verified. The operation is idempotent. If we repeat the operation with the same data, no additional side effects occur. We can handle the failure or take compensative actions Refer\nhttps://en.wikipedia.org/wiki/Consistency_model#Client-centric_consistency_models http://www.allthingsdistributed.com/2007/12/eventually_consistent.html https://www.enterpriseintegrationpatterns.com/ramblings/18_starbucks.html [Must need to check and he suggest one book need to check] [Life Beyond Distributed Transactions: An Apostate’s Opinion] https://queue.acm.org/detail.cfm?id=3025012 https://etcd.io/ [] Macro Architecture: Handling Security Interaction Security Authentication Techniques\nhttps://github.com/ory/keto. Macro Architecture: Handling High Availability and Scale Before going to scale the system to N Do the POC of single system capacity how much they can handle and if possible can tweak that to improve performance.\nThere are four tactics (techniques) to keep communication low, and they help us scale.\nShare Nothing Distribution Caching Async Processing Refer\nScalability! But at What COST? (https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf) Macro Architecture: Microservices Considerations Microservices let us split the structure into many loosely connected parts that can be developed, released, improved, and managed independently.\nThe decisions to make before going microservice\nHandling Shared Database(s)\nEach microservice should have its own database, and two microservices must not share data via the same database. This rule removes a common cause that leads to tight coupling between services If we want to share then make sure only one Service do update to avoid data reduntency Use transaction if we want both service to update Securing Microservices\nCoordinating Microservices\nAvoiding Dependency Hell\nOne service depolyment not to break other service if we have dependency use Backward Compatibility or Forward Compatibility Backward Compatibility: If we have updated the API to v2 which will accpet query params but make sure that API support without query params that\u0026rsquo;s how before it work\u0026rsquo;s. Forward Compatibility: If V2 not working try V1 (it just temporary solution) Avoid dependency hell by Be conservative in what you do, be liberal in what you accept from others Refer\nhttp://martinfowler.com/articles/microservices.html http://martinfowler.com/articles/microservice-trade-offs.html http://martinfowler.com/bliki/MicroservicePremium.html https://cramonblog.wordpress.com/2014/02/25/micro-services-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-1/ https://www.infoq.com/news/2015/06/taming-dependency-hell/ https://news.ycombinator.com/item?id=9705098 Server Architectures some guidelines for writing efficient and simple services\nDo not reinvent the wheel Use pools to reuse complex objects (need to see how moongose have pool of connections what they doing) Make service operations idempotent whenever possible Service Architecture Thread-per-Request Architecture Event-Driven (Nonblocking) Architecture Staged Event-Driven Architecture (SEDA) classification of applications based on their resource usage\nCPU-bound applications (CPU \u0026raquo; Memory and no I/O) Memory-bound applications (CPU + Bound Memory and no I/O) Balanced applications (CPU + Memory + I/O) I/O-bound applications (I/O \u0026gt; CPU) Refer\nhttps://stackoverflow.com/questions/3570610/what-is-seda-staged-event-driven-architecture. https://berb.github.io/diploma-thesis/ https://mechanical-sympathy.blogspot.com/2011/09/single-writer-principle.html. https://mechanical-sympathy.blogspot.com/2011/07/memory-barriersfences.html. https://martinfowler.com/bliki/CQRS.html Building Stable Systems scenarios that affect stability:\nUnexpected workloads Resource and dependency failures, plus service-level agreement violations Software bugs in both ours and borrowed code Handling Unexpected Load understand the load and set up a system that can keep up with the load most of the time. This is called capacity planning Autoscaling Admission Control: set of policies, procedures, and checks that regulate the admission or acceptance of incoming entities or requests into a system example: Assume server is already overloaded instead of accepting new connection we through error. Noncritical Functionality: Turn off the Noncritical Functionality Disaster recovery (have a plan for what to when X go down) Handling Human Changes Blue and green Deployment (Having two system one with new updated one and one with old one if new one fails redirect traffic to old one) Canary deployments (small percentage of users to the new system and gradually increase the load to that system) Handle Unknown Errors Observability (Application Performance Monitoring (APM) tool, Open telementry) Building and Evolving the Systems Get the Basics Right Understand the Design Process Conway law: organizations design systems that mirror their own communication structure rather than fighting it Make Decisions and Absorb Risks Create checklists of questions that are useful for different situations and use them. Communicating the Design Growth hacking funnel (find where we stuck does in user side or in development are we not pusing not much feature etc..) Refer\nhttps://blog.thinkst.com/2022/08/always-be-hacking.html. Book\nThe Coaching Habit: Say Less, Ask More \u0026amp; Change the Way You Lead Forever Hacking Growth: How Today’s Fastest-Growing Companies Drive Breakout Success Design-Driven Growth: Strategy \u0026amp; Case Studies for Product Shapers My Learnings ","permalink":"https://programmerraja.github.io/blog/notes/2024/software-architecture-and-decision-making/","summary":"Book Core concepts It explains principles and concepts I believe a senior architect must understand deeply and discusses how to employ those principles to manage uncertainty\nBooks for leadership The Hard Things About Hard Things by Ben Horowitz, Trillion Dollar Coach by Eric Schmidt et al., Team of Teams: New Rules of Engagement for a Complex World by Stanley McChrystal Good Strategy, Bad Strategy by Richard Rumel Three layers of architecture of The Open Group Architecture Framework (TOGAF)","title":"Software Architecture and Decision-Making"},{"content":"The Nature of Complexity Complexity is anything related to the structure of a software systemthat makes it hard to understand and modify the system.\nSymptoms of complexity Change amplification: The first symptom of complexity is that a seemingly simple change requires code modifications in many different places. Cognitive load: how much a developer needs to know in order to complete a task Unknown unknowns: it is not obvious which pieces of code must be modified to complete a task, or whatinformation a developer must have to carry out the task successfully Strategic vs. Tactical Programming Tactical programming In the tactical approach, your main focus is to get something working, such as a new feature or a bug fix.\nStrategic programming Strategic programming requires an investment mindset. Rather than taking the fastest path to finish your current project, you must invest time to improve the design of the system.\nModules Should Be Deep A software system is decomposed into a collection of modules that are relatively independent.\nDeep modules A deep module are the modules that take care of the most of things The mechanism for file I/O provided by the Unix operating system and its descendants, such as Linux, is a beautiful example of a deep interface.\nThere are only five basic system calls for I/O, with simple signatures:\nint open(const char* path, int flags, mode_t permissions); ssize_t read(int fd, void* buffer, size_t count); ssize_t write(int fd, const void* buffer, size_t count); off_t lseek(int fd, off_t offset, int referencePosition); int close(int fd); The abstraction will take care of bellow thing\nHow are files represented on disk in order to allow efficient access? How are directories stored, and how are hierarchical path names processed to find the files they refer to? How are permissions enforced, so that one user cannot modify or delete another user’s files? How are file accesses implemented? For example, how is functionality divided between interrupt handlers and background code, and how do these two elements communicate safely? What scheduling policies are used when there are concurrent accesses to multiple files? How can recently accessed file data be cached in memory in order to reduce the number of disk accesses? Don\u0026rsquo;t expose the thing that no need for user. try to abstract the things as much as possible\nInformation Hiding (and Leakage) Information hiding The basic idea is that each module should encapsulate a few pieces of knowledge, which represent design decisions. The knowledge is embedded in the module’s implementation but does not appear in its interface, so it is not visible to other modules.\nexamples of information that might be hidden within a module (it nit about using private method .it is about hiding the hard implementation from user)\nHow to store information in a B-tree, and how to access it efficiently. How to identify the physical disk block corresponding to each logical block within a file. How to implement the TCP network protocol. How to schedule threads on a multi-core processor. How to parse JSON documents. Information leakage when a design decision is reflected in multiple modules. This creates a dependency between the modules: any change to that design decision will require changes to all of the involved modules\nExample: Imagine a scenario in a document processing application where there are two classes: one for reading documents from various file formats and another for writing documents in those formats.\nclass DocumentReader: def __init__(self, file_path): # Initialization logic def read_document(self): # Read document logic pass # Other methods related to reading documents class DocumentWriter: def __init__(self, file_path): # Initialization logic def write_document(self, document): # Write document logic pass # Other methods related to writing documents In this design, both DocumentReader and DocumentWriter classes have knowledge of specific file formats. If the file format changes, both classes would need to be modified, leading to information leakage. The dependency on the file format is not explicitly part of the interface, but it exists implicitly in the implementation details.\nImproved Design to Mitigate Information Leakage:\nclass DocumentProcessor: def __init__(self, file_path): # Initialization logic def read_document(self): # Read document logic pass def write_document(self, document): # Write document logic pass # Other methods related to document processing Temporal decomposition\nCommon cause for the information leakage. Consider an application that reads a file in a particular format, modifies the contents ofthe file, and then writes the file out again. With temporal decomposition, this application might be broken into three classes: one to read the file,another to perform the modifications, and a third to write out the new version. Both the file reading and file writing steps have knowledge about the file format, which results in information leakage.\nNote: When designing modules, focus on the knowledge that’s needed to perform each task, not the order in which tasks occur.\nGeneral-Purpose Modules are Deeper Make classes somewhat general purpose general-purpose interface could potentially be used for other purposes besides what it supposed to do think for future how it will adpat if we want to changes.Generality leads to better information hiding\nspecial purpose interface are the inteface only for solving current specific problem it won\u0026rsquo;t for future case.\nQuestions to ask yourself What is the simplest interface that will cover all my current needs? In how many situations will this method be used? (if it is one time then it is special purpose. try to combine multipl special purpose. in to single genral purpose) Is this API easy to use for my current needs? Different Layer, Different Abstraction Pass-through methods // Service Layer public class BusinessLogicService { public void performComplexOperation(String data) { // Complex business logic implementation System.out.println(\u0026#34;Performing complex operation with data: \u0026#34; + data); } } // Facade Layer (Pass-through layer) public class FacadeLayer { private BusinessLogicService businessLogicService; public FacadeLayer() { this.businessLogicService = new BusinessLogicService(); } public void exposeSimilarOperation(String data) { // Pass-through method businessLogicService.performComplexOperation(data); } } // Client or User Interface Layer public class UserInterface { public static void main(String[] args) { FacadeLayer facadeLayer = new FacadeLayer(); // Client invokes a method on the facade layer facadeLayer.exposeSimilarOperation(\u0026#34;Some data\u0026#34;); } } While this design might be reasonable in some cases, it can lead to problems when the facade layer becomes a mere pass-through, offering little additional value. In situations like this:\nThe facade layer adds minimal or no logic of its own. The client could potentially interact directly with the service layer, bypassing the facade layer. This scenario might indicate a lack of clear separation of concerns or an unnecessary abstraction layer. If the facade layer doesn\u0026rsquo;t add meaningful functionality or abstraction beyond simply invoking methods from the service layer, it might be worth reconsidering the design to ensure a more effective and maintainable architecture. The goal is to have each layer in the system contribute value and have a clear purpose, rather than serving as a simple pass-through.\nPass-through variables Variable that is passed down through a long chain of methods.Pass-through variables add complexity because they force all of the intermediate methods to be aware of their existence, even though the methods have no use for the variables.\nSolution: use context (A context stores all of the application’s global state)\nPull Complexity Downwards Suppose that you are developing a new module, and you discover a piece of unavoidable complexity. Which is better: should you let users of the module deal with the complexity, or should you handle the complexity internally within the module?\nIf the complexity is related to the functionality provided by the module handle internally else the users to handle the complexity.\nExample: Avoid Configuration parameters for the module even though it give control to users but In many cases, it’s difficult or impossible for users or administrators to determine the right values for the parameters . consider network protocol where it implemented the retry logic with perodic time by analysing own\nBefore exporting a configuration parameter, ask yourself will users (or higher-level modules) be able to determine a better value than we can determine here? Better Together Or Better Apart? If the pieces are unrelated, they are probably better off apart. Here are a few indications that two pieces of code are related:\nThey share information; for example, both pieces of code might depend on the syntax of a particular type of document They are used together: anyone using one of the pieces of code is likely to use the other as well and vice versa and make sure the module not used by other if it can be used by multiple it need to be sepreate. It is hard to understand one of the pieces of code without looking at the other. Repetition (RED FLAG)\nIf the same piece of code (or code that is almost the same) appears over and over again, that’s a red flag that you haven’t found the right abstractions.\nSpecial-General Mixture (RED FLAG)\nwhen a general-purpose mechanism also contains code specialized for a particular use of that mechanism. This makes the mechanism more complicated and creates information leakage between the mechanism and the particular use case: future modifications to the use case are likely to require changes to the underlying mechanism as well.\nSplitting and joining methods Methods containing hundreds of lines of code are fine if they have a simple signature and are easy to read. These methods are deep (lots of functionality, simple interface), which is good.\nIf a method has all of these properties, then it probably doesn’t matter whether it is long or not\nEach method should do one thing and do it completely The method should have a simple interface, so that users don’t need to have much information in their heads in order to use it correctly. its interface should be much simpler than its implementation Example: we used to write a wrapper for loggin purpose but it just one line instead of writing that we can directly log where it needed.\nWhen spliting large function in to small function make sure the below things\nsomeone reading the child method doesn’t need to know anything about the parent method and vice versa. (child method is relatively general-purpose) Conjoined Methods (RED FLAG)\nIt should be possible to understand each method independently. If you can’t understand the implementation of one method without also understanding the implementation of another\nDefine Errors Out Of Existence The best way to eliminate exception handling complexity is to define your APIs so that there are no exceptions to handle: define errors out of existence\nExample: File Deletion in window throw error if file used by some other process. but in linux it will not it marked for deletion and wait for the process to compelete and once it done the file get removed.\nJava substring function will throw error if the index is out of range it better the function to hadle the case and return accordingly.\nMask exceptions Reducing the number of places where exceptions must be handled is exception masking.With this approach, an exceptional condition is detected and handled at a low level in the system, so that higher levels of software need not be aware of the condition.\nExample: NFS network file system will not throw error if the NFS server crashed it will retry until it get connected or the threshold time reach.\nNOTE: Exception masking doesn’t work in all situations, but it is a powerful tool in the situations where it works.\nException aggregation exception aggregation is to handle many exceptions with a single piece of code; rather than writing distinct handlers for many individual exceptions, handle them all in one place with a single handler.\nDesign it Twice your first thoughts about how to structure a module or system will produce the best design. You’ll end up with a much better result if you consider multiple options for each major design decision design it twice.\nWhy Write Comments? The Four Excuses the process of writing comments, if done correctly, will actually improve a system’s design\nGood code is self-documenting Comments get out of date and become misleading I don’t have time to write comments All the comments I have seen are worthless Comments Should Describe Things that Aren’t Obvious from the Code Developers should be able to understand the abstraction provided by a module without reading any code other than its externally visible declarations. The only way to do this is by supplementing the declarations with comments.\nhow to write good comments\nPick conventions Don’t repeat the code Lower-level comments add precision (Some comments provide information at a lower, more detailed, level than the code; these comments add precision by clarifying the exact meaning of the code) Higher-level comments enhance intuition Implementation comments: what and why, not how Choosing Names Names should be precise Use names consistently Avoid extra words Write The Comments First Use Comments As Part Of The Design Process\nModifying Existing Code If you want to maintain a clean design for a system, you must take a strategic approach when modifying existing code. Ideally, when you have finished with each change, the system will have the structure it would have had if you had designed it from the start with that change in mind. To achieve this goal, you must resist the temptation to make a quick fix. Instead, think about whether the current system design is still the best one, in light of the desired change. If not, refactor the system so that you end up with the best possible design. With this approach, the system design improves with every modification.\nConsistency Consistency can be applied at many levels in a system; here are a few examples.\nNames. Coding style Design patterns Ensuring consistency Create a document that lists the most important overall conventions, such as coding style guidelines. Place the document in a spot where developers are likely to see it\nDon’t change existing conventions\nResist the urge to “improve” on existing conventions. Having a “better idea” is not a sufficient excuse to introduce inconsistencies. Your new idea may indeed be better, but the value of consistency over inconsistency is almost always greater than the value of one approach over another.\nBefore introducing inconsistent behavior, ask yourself two questions\ndo you have significant new information justifying your approach that wasn’t available when the old convention was established is the new approach so much better that it is worth taking the time to update all of the old uses Code Should be Obvious If code is obvious, it means that someone can read the code quickly, without much thought, and their first guesses about the behavior or meaning of the code will be correct. If code is obvious, a reader doesn’t need to spend much time or effort to gather all the information they need to work with the code.\nThings that make code less obvious\nEvent-driven programming Generic containers Code that violates reader expectations Software Trends Object-oriented programming and inheritance Agile development (development should be incremental and iterative) Unit tests Test-driven development Design patterns Designing for Performance have a general sense for what is expensive and what is cheap, you can use that information to choose cheap operations whenever possible\nDecide What Matters Minimize what matters Thinking more broadly ","permalink":"https://programmerraja.github.io/blog/notes/2024/a-philosophy-of-software-design-book-notes/","summary":"The Nature of Complexity Complexity is anything related to the structure of a software systemthat makes it hard to understand and modify the system.\nSymptoms of complexity Change amplification: The first symptom of complexity is that a seemingly simple change requires code modifications in many different places. Cognitive load: how much a developer needs to know in order to complete a task Unknown unknowns: it is not obvious which pieces of code must be modified to complete a task, or whatinformation a developer must have to carry out the task successfully Strategic vs.","title":"A Philosophy of Software Design Book Notes"},{"content":"File system Android devices typically have a partitioned storage system, where the storage is divided into different partitions, each serving a specific purpose.\n/system: This directory contains the Android operating system and system applications. It is read-only to ensure the integrity of the system. Modifying files in this directory could potentially harm the device.\n/app: prebundled apps from Google, as well as any vendor or carrier-installed apps /bin: various daemons, as well as shell commands contain bootanimation,dalvikvm, adb, etc. /etc: Miscellaneous configuration files /framework: contained in .jar files, with their executable dex files optimized alongside them in .odex . /lib: native ELF shared object (.so) files. This directory serves the same role as /lib in vanilla Linux. /media: Alarm, notification, ringtone and UI-effect audio files in .ogg format,and the system boot animation /usr: Support files, such as unicode mappings (icudt511.dat), key layout files for keyboards and devices, etc. /vendor: Vendor specific files /data: This directory stores user data, settings, and application data. It includes subdirectories for individual apps, each identified by its package name. User-generated data, app preferences, and other user-specific information are stored here.\n/anr: Used by EVNQTUBUF to record stack traces of non-responsive Android Apps. /app: User-installed applications. Downloaded .apk files can be found here. /data: Data directories for installed applications Each application gets its own subdirectory, in reverse DNS format example com.android.providers.calendar /cache: This directory is used for storing temporary files, such as cached data from applications. Clearing the cache can help free up storage space, but it won\u0026rsquo;t delete essential app data.\n/mnt or /storage: These directories are used for mounting external storage devices like SD cards or USB drives. The actual path may vary across devices and Android versions.\n/sdcard or /storage/emulated/0: This is the default directory for the primary external storage on the device. It\u0026rsquo;s often used to store user-accessible files such as photos, videos, and downloaded content. Note that on many modern devices, the \u0026ldquo;sdcard\u0026rdquo; directory might actually be part of the internal storage and not necessarily refer to an external SD card.\n/proc: This virtual directory contains information about system processes. It provides a way to access real-time information about the system and running processes.\n/dev: This directory contains device files representing hardware components and peripheral devices. It allows communication between the kernel and these devices.\n/sbin and /bin: These directories contain essential system binaries and executable files needed for the device\u0026rsquo;s basic functionality.\n/etc: This directory contains configuration files used by the system and various applications.\n/lib: This directory contains shared libraries needed by the system and apps. These libraries provide essential functions and services.\n/vendor: This directory contains files related to the device\u0026rsquo;s hardware, including proprietary drivers and firmware.\nboot process Bootloader (Boot ROM): When you power on the Android device, the Boot ROM (Read-Only Memory) or bootloader is the first piece of code that runs. The bootloader is responsible for initializing hardware components, checking for the connected peripherals, and loading the next stage of the boot process. Bootloader Stage 2: The second stage of the bootloader is often responsible for loading the Android kernel into memory. It may also initialize the root file system and prepare for the transition to the Linux kernel. Linux Kernel Initialization: Android is built on the Linux kernel. Once the bootloader hands control over, the Linux kernel is loaded into memory. The kernel initializes the device\u0026rsquo;s hardware, such as the CPU, memory, display, input devices, and various peripherals. Init Process: The init process is the first user-space process started by the Linux kernel. It has process ID 1 and is responsible for initializing the Android system. The init process reads the init.rc file, which contains instructions for initializing various system properties and starting essential system services. Zygote and System Server: The Zygote process is a special system process that serves as a template for creating new application processes. It helps to speed up the launch of apps by preloading some common resources. The system server is started by the init process and manages core system services like the package manager, window manager, and telephony services. Android Runtime (ART or Dalvik): The Android Runtime (ART) or Dalvik (in older versions) is responsible for running Android applications. ART is the default runtime as of Android 5.0 (Lollipop). The runtime loads and executes the bytecode of Android apps. System Services and User Interface: As the system server starts, it initiates various system services that are essential for the functioning of the Android operating system. The user interface components, including the home screen and launcher, are also started during this phase. App Launch: Finally, the Android device is ready for use, and the user can interact with the system. Apps can be launched, and the user interface becomes responsive. Android Debug bridge CMD line tool it allows developers to communicate with and control Android devices over a USB connection or a TCP/IP network connection. ADB is a versatile tool that provides a wide range of functionalities for debugging, installing and uninstalling apps, copying files, and more.\nadb is a client-server tool that includes three main components:\na client that runs on your development machine and sends commands. You can execute it from a command line by running an adb command. a daemon (adbd) that runs as a background process on each device and executes commands on a device. a server that manages communication between the client and the daemon, it runs as a background process on your development machine. How to connect Install adb pkg in linux by sudo apt install adb The FastBoot Protocol ","permalink":"https://programmerraja.github.io/blog/notes/2024/android-internal/","summary":"File system Android devices typically have a partitioned storage system, where the storage is divided into different partitions, each serving a specific purpose.\n/system: This directory contains the Android operating system and system applications. It is read-only to ensure the integrity of the system. Modifying files in this directory could potentially harm the device.\n/app: prebundled apps from Google, as well as any vendor or carrier-installed apps /bin: various daemons, as well as shell commands contain bootanimation,dalvikvm, adb, etc.","title":"Android internal Notes"},{"content":"","permalink":"https://programmerraja.github.io/blog/notes/2024/linux-notes/","summary":"","title":"Linux Notes"},{"content":"Docker Engine When you install Docker, you get two major components:\nDocker client →is a command-line tool used to interact with the Docker Engine, Docker daemon (sometimes called “server” or “engine”) In a default Linux installation, the client talks to the daemon via a local IPC/Unix socket at /var/run/docker.sock The Docker engine is the core software that runs and manages containers.The Docker Engine is made from many specialized tools that work together to create and run containers APIs, execution driver, runtime (create containers) , shims, containerd (to manage container lifecycle operations — start | stop | pause | rm.)\nWhat happen when run a cmd docker container run --name ctr1 -it alpine:latest sh\nWhen you type commands like this into the Docker CLI, the Docker client converts them into the appropriate API payload and POSTs them to the correct API endpoint.\nThe API is implemented in the daemon. The daemon communicates with containerd via a CRUD-style API over gRPC to create container\ncontainerd cannot actually create containers. It uses runc to do that. It converts the required Docker image into an OCI bundle and tells runc to use this to create a new container.(it forks a new instance of runc for every container it creates.)\nHow it’s implemented on Linux\ndockerd (the Docker daemon) docker-containerd (containerd) docker-containerd-shim (shim) docker-runc (runc) Builders A builder is a BuildKit daemon that you can use to run your builds. BuildKit is the build engine that solves the build steps in a Dockerfile to produce a container image or other artifacts.\nStarting from Docker 18.09, BuildKit is included\nBuildKit is an advanced and modular container image building tool that is part of the Docker ecosystem. It is designed to improve the performance, flexibility, and security of building container images. BuildKit introduces features like parallelization, caching improvements, and the ability to use custom frontends, which allows for more efficient and customizable image building processes.\nwe need to enable it explicitly. You can do this by setting the DOCKER_BUILDKIT environment variable to 1. export DOCKER_BUILDKIT=1\nDOCKER_BUILDKIT=1 docker build . —no-cache → this cmd use the docker build kit and pull the layer parallely and faster then normal build. you can see the build output is running parallely\nTo create a multi-platform Docker image that can run on Windows, Linux, and macOS, you can use the buildx command,**docker buildx build -t your-app-image --platform linux/amd64,windows/amd64,darwin/amd64 .**\nBefore buildKit when we building docker image by setting cwd . the hole repo will send to docker engine expect docker ingore file but in buildkit the engine will request for the file it needed\nIn docker desktop default it use builkit.\nFeatures in buildKit Mount Custom front end where we can use go language and other supported language to write the docker file Images Images are made up of multiple layers that get stacked on top of each other and represented as a single object.\nA Docker image is just a bunch of loosely-connected read-only layers. To inspect the image with the docker image inspect command\ndocker image inspect ubuntu:latest\nWill print all layers in the image Docker employs a storage driver (snapshotter in newer versions) that is responsible for stacking layers and presenting them as a single unified filesystem. Examples of storage drivers on Linux include AUFS , overlay2 , devicemapper , btrfs and zfs . As their names suggest, each one is based on a Linux filesystem or block-device technology, and each has its own unique performance characteristics.\nSharing image layers: If we downloading the image and that has base layer is unbuntu which we already have it will reuse it\ndigests of images: Every time you pull an image, the docker image pull command will include the image’s digest (cryptographic content hash) as part of the return code. You can also view the digests of images in your Docker host’s local repository by adding the \u0026ndash;digests flag to the docker image ls command\ndistribution hash: When we pushing and pulling image to hub we send the layer in compressed manner. which cause the image digest to change so we use distribution hash. which is hash value of compressed image\ndangling images: A dangling image is an image that is no longer tagged, and appears in listings as none:none to get docker image ls --filter dangling=true\nmanifest list: User may have different architectures, such as Windows, ARM, and s390x. so when we pulling the image with tag we need to download the our architectures suitable image for that we use manifest list\nmanifest list contain entries for each architecture the image supports with same tag.\nWhen we pull an image, your Docker client makes the relevant calls to the Docker Registry API running on Docker Hub. If a manifest list exists for the image, it will be parsed to see if an entry exists for Linux on ARM. If an ARM entry exists, the manifest for that image is retrieved and parsed for the crypto ID’s of the layers that make up the image. Each layer is then pulled from Docker Hub’s\nArgs Containers A container is the runtime instance of an image. docker container run image --name\nStopping containers gracefully: Docker container stop sends a SIGTERM signal to the PID 1 process inside of the container. As we just said, this gives the process a chance to clean things up and gracefully shut itself down.If it doesn’t exit within 10 seconds, it will receive a SIGKILL.\nSelf-healing containers with restart policies: The following restart policies exist\nalways Will restart the container once docker is resarting (systemlctl restart docker) unless-stopped on-failed will restart a container if it exits with a non-zero exit code Containerizing an App\n# Use an official Node.js runtime as a base image FROM node:14 # Set the working directory in the container WORKDIR /usr/src/app # Copy package.json and package-lock.json to the container COPY package*.json ./ # Install the application dependencies RUN npm install # Copy the application code to the container COPY . . # Expose the port the app runs on EXPOSE 3000 # Define the command to run your application CMD [\u0026#34;node\u0026#34;, \u0026#34;app.js\u0026#34;] docker build -t app-image . docker run -p 3000:3000 -d app-image Memory \u0026amp; CPU Constraints By default it take the host cpu and memory as much it need docker run —-memory 60m my-image:tag 60Mb limit Docker have OOM killer (out of memory killer) which is enabled by default what it do was when ever the container is getting out of memory it wil the process that consume more memory to disable the OOM docker run -m 128m --oom-kill-disable my-image:tag Swap: Swap is an extension of physical memory (RAM) that allows the operating system to use a portion of the disk as if it were additional RAM. docker run --memory=512m --memory-swap=1g my_container swap space is typically enabled by default. This means that the operating system may use swap space to store less frequently accessed data when the physical memory (RAM) is fully utilized. If we set memory and swap same it won\u0026rsquo;t use swap Note: Swap may downgrad the performance of the host docker run --cpus .5 my-image:tag can use only 50% of the CPU docker run —-cpuset-cpus 1,2 my-image:tag allocate the 2nd and 3rd CPU Enviroment Var docker run -e VARIABLE_NAME=variable_value my_container docker inspect --format='{{range .Config.Env}}{{println .}}{{end}}' container_id to inspect the environment variables of a running container. Logs docker logs [container_name_or_id] docker events —-since ‘5m’ docker logs -f my-container follow logs docker run -D Run in debugging mode Networking Docker networking comprises three major components:\nThe Container Network Model (CNM) →is a specification that defines how container runtimes like Docker should provide networking for containers. libnetwork →is the library that implements the CNM specifications. It’s written in Go, and implements the core components outlined in the CNM. Drivers →drivers are plugins that implement the CNM specifications through libnetwork The Container Network Model (CNM)\nit defines three building blocks\nSandboxes →is an isolated network stack. It includes; Ethernet interfaces, ports, routing tables, and DNS config. Endpoints→are virtual network interfaces (E.g. veth ). Like normal network interfaces, they’re responsible for making connections. In the case of the CNM, it’s the job of the endpoint to connect a sandbox to a network. Networks →are a software implementation of an 802.1d bridge (more commonly known as a switch). As such, they group together, and isolate, a collection of endpoints that need to communicate. Drivers Driver Description bridge The default network driver. host Remove network isolation between the container and the Docker host. none Completely isolate a container from the host and other containers. overlay Overlay networks connect multiple Docker daemons together. ipvlan IPvlan networks provide full control over both IPv4 and IPv6 addressing. macvlan Assign a MAC address to a container. By default docker have bridge network(in windows it called NAT) which assign a unique ip address for each container from the range of 172.17.0.0/16\nThe default “bridge” network, on all Linux-based Docker hosts, maps to an underlying Linux bridge in the kernel called “docker0” docker network inspect bridge | grep bridge.name to check the docker network inspect (docker network inspect bridge) the container.\nHow to make two docker to communicate among them\nCreate a network docker network create -d nat localnet Run two container on same network docker container run --network localnet name Communicate with docker name in container ping containername1 Note: The default bridge network on Linux does not support name resolution via the Docker DNS service.\nbridge Network we can create a custom bridge using docker create network name which will create the new netowork bind with bridge and allocate the new Ip range and if you want the container to run on this network attach with —network=name\nHost Network It directly bind the container to host and exposed to access publicly docker run --network host name we can access this by host IP.\nMACVLAN Connect the container interface through to the hosts interface.it requires the host NIC to be in promiscuous mode (isn’t allowed on most public cloud platforms)\nVolumes If you want your container’s data to stick around (persist), you need to put it on a volume. Volumes are decoupled from containers, meaning you create and manage them separately, and they’re not tied to the lifecycle of any container. Net result, you can delete a container with a volume, and the volume will not be deleted.\ndocker volume create myvol\ndocker volume inspect myvol\n[ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2018-01-12T12:12:10Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: {}, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/myvol/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;myvol\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] Mountpoint → where the data will be stored By default, Docker creates new volumes with the built-in local driver. As the name suggests, local volumes are only available to containers on the node they’re created on. Use the -d flag to specify a different driver. Third-party drivers are available as plugins. These can provide advanced storage features, and integrate external storage systems with Docker\nTypes Host volume → we explicitly tell the host path and container path Anonymous voulme → we only tell the container path it will automatically mount the data to /var/lib/docker/volumes/random_hash/data Named volume → Create volume using docker volume create myvol and attach it when running docker run -v myvol:path_in_container docker run —name myapp -p 4000:4000 -v full_path_in_current_host:path_in_docker run imagename\nwe can use this to avoid rebuiliding the images for each file change in our code we can directly mount the current code dir to docker container which will help to avoid building again and again. use this only for dev.\nMultistage Build This will be used to reduce the size of docker image by removing the unwanted things that no need for running the appliaction.let us assume you have program that can be compiled in to bin so we don’t need the things that we used during building the program.\nExample : In nodejs typescript code we no need the src code after compilling in to js so we can just copy the necessary file and remove other things.\n# Stage 1: Build Stage FROM node:latest AS build WORKDIR /app # Copy package.json and package-lock.json COPY package*.json ./ # Install dependencies RUN npm install # Copy source code COPY . . # Build the application RUN npm run build # Stage 2: Production Stage (new base layer) #FROM will start a new base layer old will be ingored FROM nginx:latest # Copy built files from the build stage to the production image #(the above will be considered as seprate image and removed) COPY --from=build /app/dist /usr/share/nginx/html # Other configurations, if needed # Container startup command for the web server (nginx in this case) CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] docker image history image name → to print all layers and how it was builded\nSecurity Scanning for vulernablity Trivy: Lightweight and easy to use. Clair: Part of the CoreOS project, designed for container scanning. Dagda: Extensive vulnerability scanner for Docker containers. Anchore: Provides detailed analysis and policy enforcement. docker scan \u0026lt;image_name\u0026gt;:\u0026lt;tag\u0026gt; part of Docker Hub and is designed to analyze Docker images for security vulnerabilities. Docker Compose It was a Python tool (v1) that sat on top of Docker, and allowed you to define entire multi-container apps in a single YAML file. now v3 written in GO\nversion:3 services: web: #name for the container build: . #context if we have docker file give the path here and not give image image: imagename enviroment: DB_URL: value ports: - \u0026#34;8000:5000\u0026#34; volumes: - .:/code #(map current dir (.) to the /code dir in container) depends_on: - redis #will wait for redis to spin up restart: always redis: image: redis Compose file Structure\nversion: Specifies the version of the Docker Compose file syntax. It ensures compatibility and defines which features are available. **** Services Describes the containers that make up the application, specifying their configuration, links, and other details. image: Specifies the Docker image to use for the service. It can be an official image from a registry or a custom image. build: Defines the build context for the service, allowing you to build a custom image from a Dockerfile. ports: Maps container ports to host ports, allowing external access to the service. volumes: Mounts volumes from the host or other containers into the service, providing persistent storage. environment: Sets environment variables for the service, influencing its runtime behavior. depends_on: Specifies dependencies between services, ensuring one service starts only after its dependencies are up. networks: Connects the service to specific networks, facilitating communication with other services. command: Overrides the default command specified in the Docker image, allowing custom command execution. entrypoint: Similar to command, but it specifies the entry point for the container. restart: Defines the restart policy for the service, determining how the container behaves after it exits. networks Defines networks that containers can connect to. This allows you to isolate containers or facilitate communication between them. volumes: Declares named volumes that can be mounted into containers. Volumes persist data beyond the lifetime of a container. configs: Specifies configuration files for services. It allows you to manage configuration separately from the Compose file. secrets: Defines secrets that can be used by services. It helps manage sensitive data securely. extensions: Provides a way to extend the Compose file by referencing external Compose files. Docker compose CMDs\ndocker-compose up: Builds, (re)creates, starts, and attaches to containers as per the configuration defined in the docker-compose.yml file. docker-compose down: Stops and removes containers, networks, volumes, and images created by docker-compose up. docker-compose build: Builds or rebuilds services specified in the docker-compose.yml file. it will build all images with prefix with name of directory docker-compose ps: Lists the running containers associated with the Docker Compose configuration. docker-compose logs: Displays log output from services. You can specify the service name to view logs for a specific service. docker-compose exec: Runs a command in a running service container. Useful for executing one-off commands or accessing a shell within a container. docker-compose stop: Stops running services defined in the docker-compose.yml file without removing them. docker-compose start: Starts stopped services defined in the docker-compose.yml file. docker-compose restart: Restarts services. This is equivalent to stopping and then starting services. docker-compose down -v: Stops and removes containers, networks, volumes, and images, including volumes. Useful for a complete cleanup. Best practices caching Make sure the order is correct such that it will do caching docker will not apply caching after one non caching stage COPY ./app RUN apt-get install RUN apt-get install In above example COPY will always change if we make a code change which cause the doker to not use cache for the upcoming layer so move that to last as possible Remove the pkg that no need Resources https://medium.com/datamindedbe/how-we-reduced-our-docker-build-times-by-40-afea7b7f5fe7 https://btholt.github.io/complete-intro-to-containers/ Tools A Docker + Kubernetes network trouble-shooting swiss-army container ","permalink":"https://programmerraja.github.io/blog/notes/2024/docker/","summary":"Docker Engine When you install Docker, you get two major components:\nDocker client →is a command-line tool used to interact with the Docker Engine, Docker daemon (sometimes called “server” or “engine”) In a default Linux installation, the client talks to the daemon via a local IPC/Unix socket at /var/run/docker.sock The Docker engine is the core software that runs and manages containers.The Docker Engine is made from many specialized tools that work together to create and run containers APIs, execution driver, runtime (create containers) , shims, containerd (to manage container lifecycle operations — start | stop | pause | rm.","title":"Docker Notes"},{"content":"Storage Engines The storage engine (or database engine) is a software component of a database man‐ agement system responsible for storing, retrieving, and managing data in memory and on disk, designed to capture a persistent, long-term memory of each node.\nStorage engines such as BerkeleyDB, LevelDB and its descendant RocksDB, LMDB and its descendant libmdbx, Sophia, HaloDB,\nRow-Oriented Data Layout Row-oriented database management systems store data in records or rows. Their layout is quite close to the tabular data representation, where every row has the same set of fields.\nrow-oriented stores are most useful in scenarios when we have to access data by row, storing entire rows together\nBecause data on a persistent medium such as a disk is typically accessed block-wise a single block will contain data for all columns. This is great for cases when we’d like to access an entire user record, but makes queries accessing individual fields of multiple user records\nColumn-Oriented Data Layout Column-oriented database management systems partition data vertically.Here, values for the same column are stored contiguously on disk\nColumn-oriented stores are a good fit for analytical workloads that compute aggregates, such as finding trends, computing average values, etc. Processing complex aggregates can be used in cases when logical records have multiple field\nAssume the table have id ,data and price which will be stored as below so if you want to calculate the total price it will be single disk read IO.\nSymbol: 1:DOW; 2:DOW; 3:S\u0026amp;P; 4:S\u0026amp;P Date:1:08 Aug 2018; 2:09 Aug 2018; 3:08 Aug 2018; 4:09 Aug 2018 Price: 1:24,314.65; 2:24,136.16; 3:2,414.45; 4:2,232.32\nIndexing An index is a structure that organizes data records on disk in a way that facilitates efficient retrieval operations.\nThere are 2 index type\nPrimary index : index created by the database itself using primary key Secondary indexes : Created by the user based on need Secondary indexes\nMostly Secondary indexes will hold the primary key index reference and primary index will hold the address for the data in memory the advantage of using this method is if update/delete is easy because updating primary index location is fine but it need 2 lookup when we searching.\nThe way the index stored is classified in 2 types\nclustered index : Where the physical order of the rows in the table corresponds to the order of the index\u0026rsquo;s keys. let assume you have username filed as primary index then the data stored in data file be sorted based on username. non clustered index: does not maintain the order of primary index Hard Disk Drives On spinning disks, seeks increase costs of random reads because they require disk rotation and mechanical head movements to position the read/write head to the desired location. However, once the expensive part is done, reading or writing contiguous bytes (i.e., sequential operations) is relatively cheap.\nThe smallest transfer unit of a spinning drive is a sector, so when some operation is performed, at least an entire sector can be read or written. Sector sizes typically range from 512 bytes to 4 Kb\nSolid State Drives SSD is built of memory cells, connected into strings (typically 32 to 64 cells per string), strings are combined into arrays,arrays are combined into pages, and pages are combined into blocks .Blocks typically contain 64 to 512 pages.\nThe smallest unit that can be written or read is a page.\nB-Tree vs B+ Tree B-Trees allow storing values on any level: in root, internal, and leaf nodes.\nB+ Trees store values only in leaf nodes all operations (inserting,updating, removing, and retrieving data records) affect only leaf nodes and propagate to higher levels only during splits and merges.\nLSM tree ![[Pasted image 20240218142044.png]]\nWrite First it writes in WAL log and appended to an in-memory component known as the memtable.when Mem-table became full, an Immutable MemTable was created, and a new Mem-table became empty. When Immutable Memtable is full, a FLUSH happens, dumping the “balanced tree in RAM” into a disk, called SSTable. which contains sorted values plus an index. SSTables are immutable, meaning once written, they are not modified. Any updates or deletes are handled by writing new SSTables with the modified data.Compaction is the process of merging and removing obsolete SSTables to reduce storage space and improve read performance. Bloom Filters LSM trees often use Bloom filters to quickly check whether a key is present in the SSTables. Bloom filters are space-efficient data structures that provide a probabilistic answer to membership queries. Resources Discover and learn about 960 database management systems Knowledge Base of Relational and NoSQL Database Management Systems Indepth\nhttps://medium.com/@hnasr/following-a-database-read-to-the-metal-a187541333c2 https://medium.com/@hnasr/database-pages-a-deep-dive-38cdb2c79eb5 ","permalink":"https://programmerraja.github.io/blog/notes/2024/database-internal/","summary":"Storage Engines The storage engine (or database engine) is a software component of a database man‐ agement system responsible for storing, retrieving, and managing data in memory and on disk, designed to capture a persistent, long-term memory of each node.\nStorage engines such as BerkeleyDB, LevelDB and its descendant RocksDB, LMDB and its descendant libmdbx, Sophia, HaloDB,\nRow-Oriented Data Layout Row-oriented database management systems store data in records or rows. Their layout is quite close to the tabular data representation, where every row has the same set of fields.","title":"Database internal book Notes"},{"content":"Questions to be asked\nHow will new service be deployed and upgraded how will it be consumed by the rest of the system Service registery -\u0026gt; Eureka\nWhere single service when new service it inform to service regis and who want to lookup it ask for service regis single point of failure we need to write code on each service Service Mesh -\u0026gt; TO solve the above problem\nuse side car and control tower (mange the sidecar ) no need to write a single code example -\u0026gt; envoy(sidecar),istio(control tower)\nEnvoy -\u0026gt; are used ton communication between service it have features like circuit breaker,load balancing,retires,\u0026hellip;etc\nit run as sidecar in container ISTIO- \u0026gt; mangae all the envoy Find Domain story telling\nAPI gateway\nAWS API TAG (Tinder API Gateway) Krakend james lewis head -\u0026gt;microservice\nAntipatterns\nways to find bounded context\nDomain story telling\nA pictorial represnetation of the domain story ","permalink":"https://programmerraja.github.io/blog/notes/2024/microservices/","summary":"Questions to be asked\nHow will new service be deployed and upgraded how will it be consumed by the rest of the system Service registery -\u0026gt; Eureka\nWhere single service when new service it inform to service regis and who want to lookup it ask for service regis single point of failure we need to write code on each service Service Mesh -\u0026gt; TO solve the above problem\nuse side car and control tower (mange the sidecar ) no need to write a single code example -\u0026gt; envoy(sidecar),istio(control tower)","title":"MicroServices"},{"content":"OSI Model The Open Systems Interconnection (OSI) model describes seven layers that computer systems use to communicate over a network.\nApplication Layer (Htttp,DHCP,FMTP,Telnet,NFS) Presentation Layer (Encryption,Data format,Data compression,ASCII to Bin) Session Layer (TCP/IP socket authentication RTCP protocol) Transport Layer (Process level addressing TCP/UDP, Multiplexing,demultiplexing,segmentation,connection establishment) Network Layer (logical addressing IP , Routing, packet encapsulation) Data Link Layer (LAN,MAC,Data framing,Error detection) Physical Layer (Converting Bin to singals to transfer the data) Application Layer HTTP/HTTPS Is the protocol used to communicate with server. it underling using TCP protocol\nHTTP Message format Start Line or Status Line GET /example HTTP/1.1 Headers General Headers (e.g., Cache-Control) Request Headers (e.g., User-Agent) Response Headers (e.g., Content-Type) Entity Headers (e.g., Content-Length) (Empty line) Message Body Request Body (for Request) / Response Body (for Response) message consists of a start-line followed by a CRLF(carriage-return and linefeed) and a sequence of octets in a format similar to the Internet Message Format example\nGET / HTTP/1.1 CRLFHost: google.comCRLFCRLF response would look like: HTTP/1.1 200 OK CRLFServer: googleCRLFContent-Length: 100CRLFtext/html; charset=UTF-8CRLFCRLF\u0026lt;100 bytes of data\u0026gt; will be decoded as Http/1.1 200 ok server:google Content-Length: 100 charset=UTF-8 \u0026lt;100 bytes of data\u0026gt; because of this we cannot do multiplexed two request need to send in ordered and response need to be in ordered.\nIf the client sends Request 1, Request 2, and then Request 3 over a single TCP connection, it will receive Response 1, Response 2, and then Response 3 in the same order.\nMethods of HTTP GET : Used to request data from a specified resource.\nPUT: Used to submit data to be processed to a specified resource.\nPOST: Used to update a resource or create a new resource if it doesn\u0026rsquo;t exist.\nDELETE: Used to request the removal of a resource.\nOPTION: The OPTIONS method is used to inquire about the communication options available for a target resource. It is often employed to check the allowed methods or capabilities of a server. Note: Whenever in browser requesting different origin(test.com) from the current webpage origin(google.com) browser first do option request to check does the origin (test.com) allowing this origin(google.com) to acess\nTRACE :The TRACE method is designed for diagnostic purposes. When a server receives a TRACE request, it echoes the received request back to the client. This can be useful for debugging and understanding how intermediaries (such as proxies or servers) modify the request.\nHEAD: Similar to GET but only requests the headers of the response, not the actual data.\nPATCH:Used to apply partial modifications to a resource\nHTTP Status code range Status Code Range Category Description 1xx Informational Request received, continuing process 2xx Successful The request was successfully received and processed 3xx Redirection Further action needs to be taken to complete the request 4xx Client Error The request contains bad syntax or cannot be fulfilled by the server 5xx Server Error The server failed to fulfill a valid request HTTP Cache HTTP caching helps reduce latency, minimize network usage, and improve the overall user experience by avoiding unnecessary resource re-fetching.\nHow Browser do caching\nBroweser do caching only if the response contain the Cache-Control header with value max-age=seconds specifies that the resource is valid for that much seconds ETag is a unique identifier assigned by the server to a specific version of a resource. When a client requests the resource again, it can include the ETag in the request headers. If the resource hasn\u0026rsquo;t changed, the server may respond with a 304 Not Modified status HTTP2 Is the improvement of the http and have some key features like below\nMultiplexing: HTTP/2 allows multiple requests and responses to be sent and received concurrently on a single connection, improving efficiency and reducing latency.\nHeader Compression: HTTP/2 uses a more efficient header compression algorithm called HPACK.\nBinary Protocol: While HTTP/1.1 uses plain text for communication, HTTP/2 employs a binary protocol. This binary format is more compact and efficient\nServer Push: HTTP/2 supports server push, which allows the server to send additional resources (like images, stylesheets, or scripts) to the client before the client requests them.\nStream Prioritization: Allows the client to indicate the priority of each resource.\nConnection Multiplexing: HTTP/2 uses a single, multiplexed connection per origin.\nHTTP/2 frames that have type, length, flags, stream identifier (ID) and payload. The stream ID makes it clear which bytes on the wire apply to which message, allowing safe multiplexing and concurrency. Streams are bidirectional. Clients send frames and servers reply with frames using the same ID. Client requests always use odd-numbered stream IDs, so subsequent requests would use stream IDs 3, 5, and so on. Responses can be served in any order, and frames from different streams can be interleaved.\nA server that is unable to establish a new stream identifier can send a GOAWAY frame so that the client is forced to open a new connection for new streams\nFormat +-----------------------------------------------+ | Frame Header | +-----------------------------------------------+ | Frame Payload | +-----------------------------------------------+ Frame Header: Represents the common structure at the beginning of each frame. It includes fields such as:\nLength: The length of the frame payload.\nType: The type of the frame (e.g., HEADERS, DATA, SETTINGS, etc.).\nHEADERS Frame (Type 0x1): Carries header fields for a particular stream. Can be used for request or response headers. PRIORITY Frame (Type 0x2): Indicates the sender\u0026rsquo;s priority weighting for a stream. Helps in managing the order of processing streams. RST_STREAM Frame (Type 0x3): Indicates that a stream is being terminated or reset. Includes an error code indicating the reason for termination. this will send when we navigate to another page browser will send to free up resources in server that no need anymore SETTINGS Frame (Type 0x4): Used to communicate configuration settings for both the client and server. Parameters include initial window size, maximum frame size, etc. PUSH_PROMISE Frame (Type 0x5): Sent by the server to initiate a server push. Specifies a promised request and the associated stream identifier. PING Frame (Type 0x6): Used to measure a round-trip time between endpoints. Primarily used as a keep-alive mechanism. GOAWAY Frame (Type 0x7): Sent by a server to indicate that further communication should not occur on a given connection. Includes information about the last stream ID processed. WINDOW_UPDATE Frame (Type 0x8): Used to adjust the size of the flow-control window. Can be sent by both the client and the server. CONTINUATION Frame (Type 0x9): Used to continue a sequence of header block fragments. Allows header information to be spread across multiple frames. DATA Frame (Type 0x0): Used to carry the payload of an HTTP message, such as the content of a request or response body. It can be associated with a specific stream. Flags: Flags providing additional information about the frame.\nEND_STREAM (0x1): Indicates that the frame represents the end of a stream. Applies to HEADERS, DATA, and CONTINUATION frames. END_HEADERS (0x4): Indicates that this frame contains the final chunk of a header set. Applies to HEADERS, CONTINUATION, and PUSH_PROMISE frames. PADDED (0x8): Indicates that the frame is followed by padding. The length of the padding is determined by the value of the Pad Length field in the frame. PRIORITY (0x20): Indicates that the frame includes stream priority information. Applies to HEADERS, PUSH_PROMISE, and CONTINUATION frames. ACK (0x1): Indicates that the SETTINGS frame acknowledges the receipt and application of the peer\u0026rsquo;s settings. Applies to SETTINGS frames. Stream Identifier: Identifies the stream to which the frame belongs.\nFrame Payload: Represents the specific content of the frame, which varies based on the frame type. For example:\nFor a HEADERS frame, the payload includes header information. For a DATA frame, the payload includes the actual data being sent. Rapid resets leading to denial of service\nThe challenge occurs when a client rapidly cancels many requests in an HTTP/2 environment, and the server or intermediary (like a proxy) struggles to promptly handle these cancellations. This can result in a buildup of tasks, causing resource consumption issues, especially in scenarios where there\u0026rsquo;s a delay in cleaning up in-process jobs. refere here\nHTTP3 It uses QUIC in transport layer\nSecurity Header in HTTP Content-Security-Policy (CSP): Defines a policy to control resource loading, mitigating risks such as cross-site scripting (XSS) attacks. Content-Security-Policy: default-src 'self'; specifies that content such as scripts, stylesheets, and images should be loaded only from the same origin. Example : let assume some how hacker inject the script tag to our website but if we have the CSP it will not be loaded by browser becasue we mentioned load only from self origin Access-Control-Allow-Origin: Specifies which origins are permitted to access the resource Access-Control-Allow-Origin: https://www.example.com this will inform the browser that only example.com can be allow to access using fetch or any API interface methods Similar to this there some other header presents like Access-Control-Allow-Methods , Access-Control-Allow-Headers, Access-Control-Allow-Credentials etc.. Strict-Transport-Security (HSTS): Forces secure connections (HTTPS) by instructing browsers to interact with the website only over secure connections. X-Content-Type-Options: Prevents browsers from interpreting files as a different MIME type, reducing the risk of certain attacks. X-Frame-Options: Controls whether a browser should be allowed to render a page in a frame, mitigating clickjacking attacks. X-Frame-Options: DENY This tell the browser that this website is not allowed to inject in iframe SAMEORIGIN This tell the browser allow iframe in the same origin X-XSS-Protection: Enables or disables the browser\u0026rsquo;s built-in Cross-Site Scripting (XSS) protection. SameSite 'strict' make sure the cookie will be send only the respective origin that set the cookie HTTPS is a secure version of HTTP.\nHow the HTTPS connection flow\nThe browser establishes a TCP (Transmission Control Protocol) connection with the web server\u0026rsquo;s IP address on port 443, which is the default port for HTTPS.\nThe browser initiates the SSL/TLS (Secure Sockets Layer/Transport Layer Security) handshake process by sending a ClientHello message to the server.\nThe web server responds with its SSL/TLS certificate, which includes the server\u0026rsquo;s public key, the server\u0026rsquo;s identity, and the digital signature of a trusted Certificate Authority (CA).\nThe browser verifies the authenticity of the server\u0026rsquo;s certificate. It checks if the certificate is signed by a trusted CA, is not expired, and matches the domain in the URL.\nThe browser generates a symmetric session key, encrypts it with the server\u0026rsquo;s public key, and sends it back to the server. This session key will be used for encrypting and decrypting data during the session.\nThe server sends a Finished message to signal that the initial handshake is complete.\nThe browser sends its own Finished message, confirming the completion of the handshake.\nWhat is Server Certificate\nThe server sends its digital certificate to the client. The certificate contains:\nPublic Key: Used for encrypting the session key. Server\u0026rsquo;s Identity (Common Name): The domain name for which the certificate was issued. Issuer: The CA that issued the certificate. Digital Signature: A signature by the CA, confirming the authenticity of the certificate. Who is CA\nA Certificate Authority is a trusted entity that issues digital certificates.The process involves the following steps:\nCertificate Request: A website owner generates a Certificate Signing Request (CSR) containing their public key and other identification information. The CSR is a file containing essential information about the website and its public key.\nopenssl genpkey -algorithm RSA -out private-key.pem will generate a public-private key pair openssl req -new -key private-key.pem -out mysite.csr With the private key create a CSR Note : LetsEncrypt scripts use OpenSSL to generate certificates and sign them with the LetsEncrypt service. CSR Submission: The website owner submits the CSR to a CA.\nVerification: The CA verifies the identity of the entity requesting the certificate. it check does we really owing the domain. they give the file that we need to serve on our domain they check does the file is serving on our domain.\nCertificate Issuance: Upon successful verification, the CA issues a digital certificate, signing it with the CA\u0026rsquo;s private key.\nDistribution to the Website: The CA sends the issued certificate to the website owner.\nWhat is SSL/TLS?\nSSL/TLS relies on public-key cryptography for secure key exchange and data encryption.it has two key public and private key public to encrypt and private to decrypt the data.\nNote: TLS is considered the more secure and modern protocol.\nSMTP Simple Mail Transfer Protocol used in the Internet to transfer electronic mail (email).\n+----------+ +----------+ +------+ | | | | | User |\u0026lt;--\u0026gt;| | SMTP | | +------+ | Client- |Commands/Replies| Server- | +------+ | SMTP |\u0026lt;--------------\u0026gt;| SMTP | +------+ | File |\u0026lt;--\u0026gt;| | and Mail | |\u0026lt;--\u0026gt;| File | |System| | | | | |System| +------+ +----------+ +----------+ +------+ SMTP client SMTP server When an SMTP client has a message to transmit, it establishes a two-way transmission channel to an SMTP server. The responsibility of an SMTP client is to transfer mail messages to one or more SMTP servers, or report its failure to do so.\nAn SMTP client determines the address of an appropriate host running an SMTP server by resolving a destination domain name to either an intermediate Mail eXchanger host or a final target host.\nThe SMTP Procedures: An Overview\nAn SMTP session is initiated when a client opens a connection to a server and the server responds with an opening message.\nOnce the server has sent the greeting (welcoming) message and the client has received it, the client normally sends the EHLO command to the server, indicating the client\u0026rsquo;s identity.\nThere are 3 steps to SMTP mail transactions.\nThe transaction starts with a MAIL command that gives the sender identification. MAIL FROM:\u0026lt;reverse-path\u0026gt; [SP \u0026lt;mail-parameters\u0026gt; ] \u0026lt;CRLF\u0026gt; . This command tells the SMTP-receiver that a new mail transaction is starting and to reset all its state tables and buffers The second step in the procedure is the RCPT command. RCPT TO:\u0026lt;forward-path\u0026gt; [ SP \u0026lt;rcpt-parameters\u0026gt; ] \u0026lt;CRLF\u0026gt; The first or only argument to this command includes a forward-path identifying one recipient. If accepted, the SMTP server returns a \u0026ldquo;250 OK\u0026rdquo; reply and stores the forward-path. The third step in the procedure is the DATA command DATA \u0026lt;CRLF\u0026gt; If SMTP accepted, the SMTP server returns a \u0026ldquo;250 OK\u0026rdquo; reply How Mail Routed to the destination\nSMTP will do DNS query to lookup for the Mail eXchanger (MX) records for the sender domain. MX records hold the IP for the mail servers that are designated to handle incoming emails for the domain.The MX records include priority values, indicating the order in which mail servers should be used. Lower values represent higher priority.\nPOP / IMAP POP (Post Office Protocol) and IMAP (Internet Message Access Protocol) are two different protocols used for retrieving emails from a mail server to a local email client.\nPOP(Post Offic Protocol): Establish connection with SMTP server and send greeting once it accepted.client server exchange the mail.it downloads emails from the server to the local device.emails are removed from the server after being downloaded.\nIMAP (Internet Message Access Protocol): allows a client to access and manipulate electronic mail messages on a server. IMAP4rev1 permits manipulation of mailboxes (remote message folders) in a way that is functionally equivalent to local folders. IMAP4rev1 also provides the capability for an offline client to resynchronize with the server.\nDNS Domain name system used to translate the domain to IP. It uses UDP protocol on port 57. First browser check does the domain corresponding IP in local cache if so use if not do the DNS Query.\nHow DNS Query get resolved\nThe resolver queries recursive DNS servers, which are typically provided by the Internet Service Provider (ISP) or a third-party service like Google\u0026rsquo;s. These servers either have the IP address for the requested domain in their cache or initiate further queries to find it. If the recursive DNS servers don\u0026rsquo;t have the information, they query root DNS servers. There are 13 sets of root DNS servers worldwide (labeled A to M), and they provide information about Top-Level Domains (TLDs) like .com, .org, .net, etc. The Root DNS servers direct the query to the authoritative DNS servers for the specific Top-Level Domain (TLD) of the requested domain. The TLD DNS servers provide information about the authoritative DNS servers for the second-level domain (e.g., \u0026ldquo;example.com\u0026rdquo;). The authoritative DNS servers or Name Server (mostly our domain provider) respond to the query with the IP address associated with the requested domain. Records in DNS\nA (Address) Record: Associates a domain name with an IPv4 address. AAAA (IPv6 Address) Record: domain name with an IPv6 address CNAME (Canonical Name) Record: alias for a domain name, pointing it to another domain\u0026rsquo;s canonical (official) name this can be used if you want a example.com domain to google.com we can add the CName record for it. MX (Mail Exchange) Record: mail servers responsible for receiving emails on behalf of the domain. TXT (Text) Record: text information associated with a domain. Often used for DNS-based verification or providing additional information. NS (Name Server) Record: Specifies authoritative DNS servers for the domain Sender policy framework (SPF) record: that lists all the servers authorized to send emails from a particular domain. when the recevier recevie mail from this domain it will check for the SPK record in sender domain address if the IP is in that list it will allow else block it or mark as spam. Note: The DNS first Query for the A Record if it exist it use the IP else it look for the CName for the domain which will have a another domain which have A Record that will hold the IP of the server\nDHCP Dynamic Host Configuration Protocol is used to dynamically assign IP addresses and other network configuration information to devices on a network.\nHow Client Communicate with DHCP\nThe client broadcasts a DHCPDISCOVER message on its local physical subnet.\nEach server may respond with a DHCPOFFER message that includes an available network address\nThe client receives one or more DHCPOFFER messages from one or more servers. The client may choose to wait for multiple responses. The client chooses one server from which to request configuration parameters, based on the configuration parameters offered in the DHCPOFFER messages.\nThe client broadcasts a DHCPREQUEST message that MUST include the \u0026lsquo;server identifier\u0026rsquo; option to indicate which server it has selected\nThe server selected in the DHCPREQUEST message commits the binding for the client to persistent storage and responds with a DHCPACK message containing the configuration parameters are default gateway IP, subnetmask,DNS server address,least time (time for IP valid)\nSSH Provides secure access to a remote device or server over a network, allowing for encrypted communication and command execution.\nWebRTC Peer to peer exchange video and audio\nSTUN (session traversal util for NAT)\nA Server tell me my public ip address/port through NAT The client want to know his public ip and port (the ip of router) The clent will do STUN request to the STUN server where it send the response of the public ip and port TURN (traversal using relay around NAT) VoIP allows voice communication and multimedia sessions over the Internet.\nVoIP devices register with a VoIP server using a protocol like SIP (Session Initiation Protocol). SIP is responsible for establishing, modifying, and terminating real-time sessions.\nTo connect with traditional phone networks, VoIP calls often go through an Internet Telephony Service Provider (ITSP). The ITSP serves as a gateway between the IP-based and traditional telephone networks.\nprotocols commonly used in VoIP\nSIP (Session Initiation Protocol): SIP is a signaling protocol used for initiating, modifying, and terminating communication sessions. It\u0026rsquo;s widely used for setting up and tearing down VoIP calls. RTP (Real-Time Transport Protocol): RTP is used for transmitting audio and video data in real-time over the internet. It works in conjunction with RTCP (Real-Time Control Protocol) for quality monitoring. SDP (Session Description Protocol): SDP is used to describe multimedia sessions for the purpose of session announcement, invitation, and other forms of initiation. RTCP (Real-Time Control Protocol): RTCP works alongside RTP to provide control information during a VoIP session, including statistics on packet loss, jitter, and round-trip delay. UDP (User Datagram Protocol): UDP is a transport protocol commonly used for carrying voice packets in VoIP due to its low-latency characteristics. It\u0026rsquo;s also used for SIP signaling. TCP (Transmission Control Protocol): While less common for voice transmission due to its connection-oriented nature, TCP is used in certain scenarios for SIP signaling and situations where retransmission of lost packets is preferred over real-time delivery. TLS (Transport Layer Security): TLS is used to secure the signaling (SIP) and media streams in VoIP, ensuring the confidentiality and integrity of communications. STUN (Session Traversal Utilities for NAT): STUN is used to discover the presence of network address translators (NATs) and to obtain the public IP address and port of a VoIP client. TURN (Traversal Using Relays around NAT): TURN is used to relay media when direct peer-to-peer communication is not possible due to NAT or firewall traversal issues. ICE (Interactive Connectivity Establishment): ICE is a framework that leverages STUN and TURN to facilitate the establishment of peer-to-peer connections in the presence of NAT and firewalls. MGCP (Media Gateway Control Protocol): MGCP is a protocol used for controlling media gateways in VoIP networks, often used in conjunction with the more modern SIP and H.323. SIP Proxy Server\nA SIP proxy receives and processes SIP requests from a redirect server or software.\nSession Initiation Protocol (SIP)\nSIP is an application-layer control protocol that can establish,modify, and terminate multimedia sessions (conferences) such as calls.\nResponses are coded based on their message. Different preceding numbers in a three-digit sequence have different meanings.\nFor example, 1xx response codes mean the device received and is processing the message. Codes starting with 2xx mean completion, 3xx is used for redirections, 4xx is for authentication errors, etc.\nThe most common code is 200, meaning the action was completed successfully without further details.\nA SIP registrar is similar to an address book. It associates the various users with the access points on the IP network where one can reach them.\nMost SIP addresses connect to a unique phone number\nunderlying media streams for audio and video calls typically use separate protocols, like RTP (Real-time Transport Protocol), which ensures timely delivery of media data.\nSIP URI It has a similar form to an email address, typically containing a username and a host name. In this case, it is sip:bob@biloxi.com, where biloxi.com is the domain of Bob\u0026rsquo;s SIP service provider.\nCodecs\nA codec, which stands for coder-decoder, converts an audio signal into compressed digital form for transmission and then back into an uncompressed audio signal for replay.\nTools Nslookup\ntool for querying the Domain Name System to obtain the mapping between domain name and IP address,\nnslookup mydomain.com -\u0026gt; Will print the IP and other DNS details nslookup -type=AAAA mydomain.com -\u0026gt; will print only AAA record Domain Information Groper command (DIG)\nfor Making DNS queries dig example.com , dig NS com will print name server of the domain\nTransport Layer It is responsible for ensuring end-to-end communication and data transfer between applications. In this layer the data is divided into small small segements and also it control the flow of the data transfer ,detecting packet losses (via sequence numbers) and errors (via per-segment checksums), as well as correction via retransmission\nThe two most commonly used transport layer protocols are:\nTCP UDP TCP Transmission Control Protocol is a connection-oriented protocol that provides reliable and ordered delivery of data between two devices. It establishes a connection before data transfer, breaks data into segments, manages acknowledgment and retransmission of data, and ensures that data is delivered without errors and in the correct order.\nsockets doors through which data passes from the network to the process and through which data passes from the process to the network.\nTCP use Port no and IP to bind the data to socket when multiple process running on the machine it uses the Port no and IP to bind with socket\nTCP protocol runs only in the end systems and not in the intermediate network elements (routers and link-layer switches), the intermediate network elements do not maintain TCP connection state.\nHow TCP Works It first perfom 3 way hadshake. The client sends a TCP segment with the **SYN **(synchronize) flag set to the server, indicating an initiation of a connection.\nThe server responds with a TCP segment containing SYN-ACK, acknowledging the request and indicating its readiness to establish a connection.\nThe client sends a final TCP segment with an ACK (acknowledge) flag set, confirming the server\u0026rsquo;s acknowledgment. The connection is now established.\nTCP Peer A TCP Peer B 1. CLOSED LISTEN 2. SYN-SENT --\u0026gt; \u0026lt;SEQ=100\u0026gt;\u0026lt;CTL=SYN\u0026gt; --\u0026gt; SYN-RECEIVED 3. ESTABLISHED \u0026lt;-- \u0026lt;SEQ=300\u0026gt;\u0026lt;ACK=101\u0026gt;\u0026lt;CTL=SYN,ACK\u0026gt; \u0026lt;-- SYN-RECEIVED 4. ESTABLISHED --\u0026gt; \u0026lt;SEQ=101\u0026gt;\u0026lt;ACK=301\u0026gt;\u0026lt;CTL=ACK\u0026gt; --\u0026gt; ESTABLISHED 5. ESTABLISHED --\u0026gt; \u0026lt;SEQ=101\u0026gt;\u0026lt;ACK=301\u0026gt;\u0026lt;CTL=ACK\u0026gt;\u0026lt;DATA\u0026gt; --\u0026gt; ESTABLISHED TCP Peer A begins by sending a SYN segment indicating that it will use sequence numbers starting with sequence number 100.\nTCP Peer B sends a SYN and acknowledges the SYN it received from TCP Peer A.\nTCP Peer A responds with an empty segment containing an ACK for TCP Peer B\u0026rsquo;s SYN\nNow the connection is established and Peer A will start sending the data to B with SYNC number\nTCP Peer A sends some data. Note that the sequence number of the segment in line 5 is the same as in line 4 because the ACK does not occupy sequence number space\n1) A --\u0026gt; B SYN my sequence number is X 2) A \u0026lt;-- B ACK your sequence number is X 3) A \u0026lt;-- B SYN my sequence number is Y 4) A --\u0026gt; B ACK your sequence number is Y Sequence number : The current sequence number Acknowledgment number is the sequence number of the next byte of data that the host is waiting for why 3 way handshake needed\nPrevention of Duplicate Connection Initiations Security Against Spoofing Flow Control and Buffer Allocation TCP Format (Segment) 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Source Port | Destination Port | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Sequence Number | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Acknowledgment Number | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Data | | | | | Offset| Rsrvd |control bits| Window | | | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Checksum | Urgent Pointer | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | [Options] | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | : : Data : : | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Control bits\nThe currently assigned control bits are CWR, ECE, URG, ACK, PSH, RST, SYN, and FIN.\nCWR: Congestion Window Reduced ECE: ECN-Echo URG: Urgent pointer field is significant ACK: Acknowledgment field is significant RST: Reset the connection SYN: Synchronize sequence numbers FIN: No more data from sender. Urgent Pointer\nis used to provide a way for the sender to inform the receiver that certain data in the segment is urgent and should be prioritized\nOptions\nMaximum Segment Size (MSS) is used to indicate the maximum size of a TCP segment that the sender can receive. Window Scale: allows the sender to advertise a larger window size than what can be represented in the standard 16-bit window field in the TCP header. Timestamps: used to measure the round-trip time and calculate the RTT (Round-Trip Time) between the sender and receiver. Selective Acknowledgment (SACK): allows a receiver to acknowledge out-of-order segments and gaps in the received data, providing more efficient error recovery and congestion control. No-Operation (NOP): is used as a placeholder for padding and does not carry any useful information. Window Size: is used to extend the window size field, providing a larger window for flow control. How TCP do reliable data transfer service TCP sender uses timout to resend the segement if the recevier does not ACK the segement that send within the specific time period.\nFlow Control\nWhen the TCP connection receives bytes that are correct and in sequence, it places the data in the receive buffer. The associated application process will read data from this buffer.\nThe receive window is used to give the sender an idea of how much free buffer space is available at the receiver.\nTCP slow start\nServers only send a few packets (typically 10) in the initial round-trip while TCP is warming up (referred to as TCP slow start). After sending the first set of packets, it needs to wait for the client to acknowledge it received all those packets. server make sure at air it won\u0026rsquo;t go above 10 that does not recevie ACK. The larger the initial window, the more we can transfer in the first roundtrip, the faster your site is on the initial page load. For a large roundtrip time .\nThe only way to estimate the available capacity between the client and the server is to measure it by exchanging data, and this is precisely what slow-start is designed to do. To start, the server initializes a new congestion window (cwnd) variable per TCP connection and sets its initial value to a conservative, system-specified value (initcwnd on Linux).\nCongestion window size (cwnd)\nSender-side limit on the amount of data the sender can have in flight before receiving an acknowledgment (ACK) from the client. The maximum amount of data in flight for a new TCP connection is the minimum of the rwnd and cwnd values; hence a modern server can send up to ten network segments to the client, at which point it must stop and wait for an acknowledgment.\nThen, for every received ACK, the slow-start algorithm indicates that the server can increment its cwnd window size by one segment — for every ACKed packet, two new packets can be sent. This phase of the TCP connection is commonly known as the \u0026ldquo;exponential growth\u0026rdquo; algorithm.\nGoogle introduced a new Algo for this which is called BBR (Bottleneck Bandwidth and Round-trip propagation time) which make YouTube network throughput by 4 percent on average globally\nTCP Congestion Control\nWhen sender send more at rate but recevier is not able to process the data at same speed the recevier buffer will be full and there is packet loss. To avoid that the recevier send the buffer size to sender i have only this much free space so send the data accordingly\nSACK Selective Acknowledgment, is an extension to the Transmission Control Protocol (TCP) designed to improve the performance of data transmission in networks with packet loss.\nSACK introduces an option in the TCP header that allows the receiver to selectively acknowledge specific segments of data that have been received successfully. This enables the receiver to inform the sender about the non-contiguous blocks of data that have been successfully received.\nSACK is an optional extension in TCP and is negotiated during the connection establishment phase. If both the sender and receiver support SACK, they can use it; otherwise, they fall back to regular TCP behavior.\nUDP User Datagram Protocol provides a procedure for application programs to send messages to other programs with a minimum of protocol mechanism.\nThe protocol is transaction oriented, and delivery and duplicate protection are not guaranteed.\nUDP bind the socket with destination IP and Port\nNo handshaking is required on UDP mostly it used for live streaming data\nFormat\n0 7 8 15 16 23 24 31 +--------+--------+--------+--------+ | Source | Destination | | Port | Port | +--------+--------+--------+--------+ | | | | Length | Checksum | +--------+--------+--------+--------+ | | data octets ... +---------------- ...---------------+ Tools Netcat\nused to create TCP or UDP connections.\nnc -v hostname or IP address port nc -l port : -\u0026gt;in listen mode to receive incoming connections. nc -l port \u0026gt; output_file: (listen on the port and store the data to file) nc ip port \u0026lt; input_file: send the file content to the host on port Tcpdump\nCaptures and analyzes network traffic. this will capture all network on the host we can use that and load to the wireshark to analyze it\ntcpdump host ip -\u0026gt; capture only from this host by defaulr all Nmap\nScans and discovers devices on a network, providing information about open ports and services. nmap [hostname or IP address]\nNetworking Layer It is responsible for logical addressing, routing, and forwarding of data between different networks. It enables end-to-end communication by determining the best path for data packets to traverse through interconnected networks.\nIP Address An IP (Internet Protocol) address is a numerical label assigned to each device. IP addresses come in two versions, IPv4 (32-bit) and IPv6 (128-bit),\nIP addresses are categorized into classes based on their leading bits\nClass A: Subnet Mask: 255.0.0.0 (or /8 in CIDR notation). Range: 1 to 126 Class B: Subnet Mask: 255.255.0.0 (or /16 in CIDR notation) 128 to 191 Class C: Subnet Mask: 255.255.255.0 (or /24 in CIDR notation) 192 to 223 Class D (Multicast): for multicast groups. 224 to 239 Class E (Reserved): for experimental purposes. 240 to 255 127 -\u0026gt; loop back Ip (localhost) Subnet Mask A subnet mask is a 32-bit number that divides an IP address into network and host portions. It is used to specify which part of the IP address represents the network and which part identifies the specific device within that network.\nIt was introduced instead of class to make it easier to find network and host.\n192.168.1.1 for this IP the subnetmask is 255.255.255.0 192.168.1.1 -\u0026gt; 11000000.10101000.00000001.00000001(bin) 255.255.255.0 -\u0026gt; 11111111.11111111.11111111.00000000 it means that the first 24 bits of the IP address are dedicated to the network, and the last 8 bits are for host addresses. so we can start from 00000000 to 11111111 (255)\nSo for the IP the network portion is 192.168.1 and host is 1 and we can add 1 to 255 computer (host) in this network\nNote 192.168.1 starting IP is act as router in the network. last IP 192.168.255 is used for broadcasting when we send data to that IP it will be sent all host belong to the network\nCIDR Notation CIDR notation represents IP addresses using a format that combines the base IP address and the number of bits used for the network portion. It follows the pattern: IP_address/prefix_length. For example, \u0026ldquo;192.168.1.0/24\u0026rdquo; means that the first 24 bits of the address are the network portion and balance 8 bit is host\nData Format (Packet) 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Ver= 4 |IHL= 5 |Type of Service| Total Length = 21 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Identification = 111 |Flg=0| Fragment Offset = 0 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Time = 123 | Protocol = 1 | header checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | source address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | destination address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | data | +-+-+-+-+-+-+-+-+ Time: This field indicates the maximum time the datagram is allowed to remain in the internet system.The router that route the packet will decrease the Time if it became zero router will drop this helps to avoid packets get loops in router it will drop once it get 0. IPV6 IPv6 increases the IP address size from 32 bits to 128 bits\nIPv6 Header Format\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| Traffic Class | Flow Label | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Payload Length | Next Header | Hop Limit | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Source Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + Destination Address + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Version : 4-bit Internet Protocol version number = 6. Hop Limit : like TTL in IPV4 (Decremented by 1 by each node that forwards the packet.) Traffic Class : ICMP Routing Protocol Routing protocols determine how your data gets to its destination it mainly classify in two types\nInterior Gateway Protocols (IGP) Exterior Gateway Protocols (EGP) Interior Gateway Protocols (IGP) These are used within an autonomous system (AS), which is a collection of routers under the same administrative control like our ISP. Examples include Routing Information Protocol (RIP) and Open Shortest Path First (OSPF).\nExterior Gateway Protocols (EGP) These are used to exchange routing information between different autonomous systems. example: Border Gateway Protocol (BGP)\nRouter Router are all over the internet helps to transmit the data from one network to another network.It has routing table which hold the IP of the known router that router and the destination.when ever router route the packet it add his IP and MAC address to the packet\nHost communicate with router using MAC address and use ARP protocol to find the MAC address of the router.\nNAT Network Address Translation allows multiple devices within a local network to share a single public IP address for communication with external networks,\nISP uses NAT where some group of users comes under the same Router where it replace his IP when the request going out of internal.\nTypes\nOne to one (packet to external IP:port on the router always maps to internal ip:port ) Address restricted NAT Port restricted Symmentric Tools Traceroute determines the route to a destination by sending Internet Control Message Protocol (ICMP) echo packets to the destination. In these packets, TRACERT uses varying IP Time-To-Live (TTL) values. Because each router along the path is required to decrement the packet\u0026rsquo;s TTL by at least 1 before forwarding the packet, the TTL is effectively a hop counter. When the TTL on a packet reaches zero (0), the router sends an ICMP \u0026ldquo;Time Exceeded\u0026rdquo; message back to the source computer.\nTRACERT sends the first echo packet with a TTL of 1 and increments the TTL by 1 on each subsequent transmission, until the destination responds or until the maximum TTL is reached.\ntracert domain -\u0026gt; will print how the packet is get transfered (the router will display * (asterisk) if no response is received.) My traceroute that combines the functionality of traceroute and ping. mtr [destination] will print all router with time it taken\nNetstat Displays network connections, routing tables, interface statistics, masquerade connections, and more.\nnetstat -a -\u0026gt; Shows all active network connections. netstat -l -\u0026gt; list all open ports netstat -r -\u0026gt; print routing table netstat -p -\u0026gt; process ID (PID) and program name associated with each network socket. netstat -s -\u0026gt; summary of various network-related statistics, such as total packets transmitted and received. Data Link Layer This layer is responsible for the framing, addressing, and error detection of data packets.The data link layer is concerned with local delivery of frames between devices on the same LAN\nEthernet Defines the rules for how devices on a local area network (LAN) communicate with each other. It is the most common technology for wired LANs and is part of the IEEE 802.3 standard.\nKey features and aspects of Ethernet:\nFrame Format: Ethernet uses a frame format for data transmission. A frame consists of various fields, including destination and source MAC addresses, a type field indicating the type of payload, the payload itself, and a Frame Check Sequence (FCS) for error detection. MAC Address: Ethernet uses Media Access Control (MAC) addresses to uniquely identify devices on a network. These addresses are assigned to network interface cards (NICs) and are crucial for addressing and forwarding Ethernet frames. Carrier Sense Multiple Access with Collision Detection (CSMA/CD): Ethernet uses a network access method called CSMA/CD. Devices on an Ethernet network listen for the presence of a carrier (other devices transmitting) before attempting to transmit. If a collision is detected, devices use a backoff algorithm before retransmitting. Hub, Switch, and Bridge: Ethernet networks can be extended using devices such as hubs, switches, and bridges. Hub: A basic networking device that broadcasts data to all devices in the network. Switch: A more intelligent device that uses MAC addresses to selectively forward frames only to the device for which the frame is intended. Bridge: Connects multiple network segments and uses MAC addresses for filtering traffic. Speed and Duplex: Ethernet supports different data rates, including 10 Mbps (Ethernet), 100 Mbps (Fast Ethernet), 1 Gbps (Gigabit Ethernet), 10 Gbps, 40 Gbps, and 100 Gbps. The duplex mode can be half-duplex (communication in one direction at a time) or full-duplex (simultaneous two-way communication). Twisted Pair Cabling: Ethernet commonly uses twisted pair cables for data transmission. Categories of twisted pair cables, such as Cat5e, Cat6, and Cat6a, support different data rates and distances. Fiber Optic Cabling: In addition to copper cables, Ethernet can operate over fiber optic cables, providing higher bandwidth and longer-distance capabilities. Ethernet Frame Types: Ethernet supports various frame types, including Ethernet II, IEEE 802.2, and IEEE 802.3. The most commonly used frame type today is Ethernet II. Ethernet over TCP/IP: Ethernet is often used as the underlying technology for TCP/IP networks. It is the foundation for local networking and Internet connectivity. Media Access Control (MAC) MAC is a unique identifier assigned to network interfaces for communication on the physical network. It is a hardware address burned into the network interface card (NIC).\nWe cannot change MAC address mostly but can be using software.MAC are used to communicate with LAN\nMAC is 48 bit. 24 bit for vendor number and 24 bit will unique for the NIC.\nARP (Address Resolution Protocol) ARP resolves an IP address to a MAC address.Each host and router has an ARP table in its memory, which contains mappings of IP addresses to MAC addresses in there LAN.\nIf the host does not have the mapping it will send ARP packet to all host in his LAN and host will return there MAC.ARP operates when a host wants to send a datagram to another host on the same subnet.\nTools ARP\nDisplays the current ARP table, which maps IP addresses to MAC addresses.\narp -a arp -s IP address MAC address -\u0026gt;Adds a static entry to the ARP table Physical Layer Responsible for the physical transmission of raw bits over a physical medium. It deals with the actual hardware connections, transmission media, and electrical or optical signaling.\nRFC RFC stands for \u0026ldquo;Request for Comments.\u0026rdquo; It is a series of documents and notes about the specifications, protocols, procedures, and conventions related to the operation and development of the internet. The RFC series is maintained by the Internet Engineering Task Force (IETF), a large open international community of network designers, operators, vendors, and researchers concerned with the evolution and smooth operation of the Internet architecture.\nThe RFC documents cover a wide range of topics, including protocols, procedures, programs, and meeting notes. Some of the most well-known and widely used internet standards, protocols, and technologies have been documented in RFCs.\nAbove Notes are mostly refered from RFC doc\nResources RFC Spec for SMTP Understanding websocket indepth Computer Networking Introduction: Ethernet and IP (Heavily Illustrated) DNS Journey from request to response Visulaize how we reach the website using traceroute Computer networking A top down approach (best book) Advanced Increase HTTP Performance by Fitting In the Initial TCP Slow Start Window How TCP slow start affect page load Try make the page size small as possible that can fit in 10 TCP segment roughly 12kb Increase the initial congestion window for the server by default for linux it has 10 . it will send 10 TCP segment parallely and it will wait for ACK for each one if it recevied it send next segment. TCP Tuning for HTTP List of methods to tunning TCP for HTTP Book High Performance Browser Networking Provides a hands-on overview of what every web developer needs to know about the various types of networks ","permalink":"https://programmerraja.github.io/blog/notes/2024/networking/","summary":"OSI Model The Open Systems Interconnection (OSI) model describes seven layers that computer systems use to communicate over a network.\nApplication Layer (Htttp,DHCP,FMTP,Telnet,NFS) Presentation Layer (Encryption,Data format,Data compression,ASCII to Bin) Session Layer (TCP/IP socket authentication RTCP protocol) Transport Layer (Process level addressing TCP/UDP, Multiplexing,demultiplexing,segmentation,connection establishment) Network Layer (logical addressing IP , Routing, packet encapsulation) Data Link Layer (LAN,MAC,Data framing,Error detection) Physical Layer (Converting Bin to singals to transfer the data) Application Layer HTTP/HTTPS Is the protocol used to communicate with server.","title":"Networking Notes"},{"content":"","permalink":"https://programmerraja.github.io/blog/notes/2024/nginx/","summary":"","title":"Nginx Notes"},{"content":"Introduction From c++ by google c++ author will convert to exe\ngo get githublink→ to download pkg and put under folder pkg\nit has main function to run go run filename.go\npackage main //-\u0026gt; our package name imort \u0026#34;fmt\u0026#34; import (\u0026#34;fmt\u0026#34;,\u0026#34;math\u0026#34;) //-\u0026gt; to import multiple pkg func main(){ fmt.Println(\u0026#34;hello\u0026#34;) } Data types string, bool, int, byte , float32 ,float64, arrays\nusing var keyword and const → to not reassign\nvar name string = “d” (if directly assing vaule we don’t need to mention the data type)\nvar age = 23\nname := “test” → this kind of declaration we can be only defined inside function\nname,email := “test”,”email”\nArrays\nvar name [2] string name[0] = \u0026#34;1\u0026#34; name := [2]string(\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;) //size optional //slice name := [2]string{\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;} len(name) //-\u0026gt; to get length name[1:2] //-\u0026gt; slice the array Struct\ntype Animal struct { class string age int } var teddy = Animal{ class:\u0026#34;bear\u0026#34;,age:14} or Animal{\u0026#34;bear\u0026#34;,14} //teddy.age Map\nemails := make(map[string]string) := map[stringstring{\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;} emails[\u0026#34;name\u0026#34;]= \u0026#34;key\u0026#34; Function function name (params type) returnType { cmd.. } //type is not mandatory name(params) Conditions if x\u0026lt;y || cond2 { //code } else if { //code } else { //code } //switch color := \u0026#34;red\u0026#34; switch color { case \u0026#34;red\u0026#34;: //code case \u0026#34;\u0026#34;: //code default: //code } Loops i :=1 for i\u0026lt;=10 { //code i++ } for i:=1 i\u0026lt;10;i++ { //code } //using range ids := []int{33,33} for i,id := range ids { } //range with map for k,v := range map { } ","permalink":"https://programmerraja.github.io/blog/notes/2024/go/","summary":"Introduction From c++ by google c++ author will convert to exe\ngo get githublink→ to download pkg and put under folder pkg\nit has main function to run go run filename.go\npackage main //-\u0026gt; our package name imort \u0026#34;fmt\u0026#34; import (\u0026#34;fmt\u0026#34;,\u0026#34;math\u0026#34;) //-\u0026gt; to import multiple pkg func main(){ fmt.Println(\u0026#34;hello\u0026#34;) } Data types string, bool, int, byte , float32 ,float64, arrays\nusing var keyword and const → to not reassign\nvar name string = “d” (if directly assing vaule we don’t need to mention the data type)","title":"Go"},{"content":"Kubernetes is two things A cluster for running applications An orchestrator of cloud-native microservices apps Kubernetes cluster A Kubernetes cluster contains six main components:\nAPI server: Exposes a REST interface to all Kubernetes resources. Serves as the front end of the Kubernetes control plane. Scheduler: Places containers according to resource requirements and metrics. Makes note of Pods with no assigned node, and selects nodes for them to run on. Controller manager: Runs controller processes and reconciles the cluster’s actual state with its desired specifications. Manages controllers such as node controllers, endpoints controllers,replica,statefulset,cron and replication controllers. Kubelet: Ensures that containers are running in a Pod by interacting with the Docker engine , the default program for creating and managing containers. Takes a set of provided PodSpecs and ensures that their corresponding containers are fully operational. Kube-proxy: Manages network connectivity and maintains network rules across nodes. Implements the Kubernetes Service concept across every node in a given cluster. Etcd: Stores all cluster data like how many pod and there replica count . Consistent and highly available Kubernetes backing store. all yaml file and current pod status are stored here Ways to create kubernetes cluster\nKind [Fast and easy] Minikube microk8s kudeadm Google cloud platform AWS Azure Nodes Nodes are the workers of a Kubernetes cluster. At a high-level they do three things:\nWatch the API Server for new work assignments\nExecute new work assignments\nReport back to the control plane (via the API server)\nNodes contain\nKubelet →When you join a new node to a cluster, the process installs kubelet onto the node. The kubelet is then responsible for registering the node with the cluster. Registration effectively pools the node’s CPU, memory, and storage into the wider cluster pool. One of the main jobs of the kubelet is to watch the API server for new work assignments. Any time it sees one, it executes the task and maintains a reporting channel back to the control plane.\nContainer runtime →The Kubelet needs a container runtime to perform container-related tasks things like pulling images and starting and stopping containers.There are lots of container runtimes available for Kubernetes. One popular example is cri-containerd .\nKube-proxy →This runs on every node in the cluster and is responsible for local cluster networking. For example, it makes sure each node gets its own unique IP address, and implements local IPTABLES or IPVS rules to handle routing and load-balancing of traffic on the Pod network.\nResources mangement\nWhen the node is running out of the resources for the pod. it will kill the pod that does not have resources mentioned in deployment file.\nPods (it’s just a sandbox for hosting containers.) The simplest model is to run a single container per Pod. However, there are advanced use-cases that run multiple containers inside a single Pod. An infrastructure-centric use-case for multi-container Pods is a service mesh.\nPod lifecycle Pods are mortal. They’re created, they live, and they die. If they die unexpectedly, you don’t bring them back to life. Instead, Kubernetes starts a new one in its place\nHow do we deploy Pods To deploy a Pod to a Kubernetes cluster you define it in a manifest file and POST that manifest file to the API Server. The control panel verifies the configuration of the YAML file, writes it to the cluster store as a record of intent, and the scheduler deploys it to a healthy node with enough available resources. This process is identical for single-container Pods and multi-container Pods.\nWe can also run the Pod directly mention the docker image kubctl run podName --image=nignix:alpline\nPods and cgroups At a high level, Control Groups (cgroups) are a Linux kernel technology that prevents individual containers from consuming all of the available CPU, RAM and IOPS on a node. You could say that cgroups actively police resource usage.Individual containers have their own cgroup limits.\nPod manifest files apiVersion: v1 kind: Pod metadata: name: hello-pod labels: zone: prod version: v1 spec: containers: - name: hello-ctr image: nigelpoulton/k8sbook:latest ports: - containerPort: 8080 The .apiVersion field tells you two things – the API group and the API version.\n.kind field tells Kubernetes the type of object is being deployed.\n.metadata section is where you attach a name and labels These help you identify the object in the cluster, as well as create loose couplings between different objects. You can also define the Namespace that an object should be deployed to. Keeping things brief, Namespaces are a way to logically divide a cluster into multiple virtual clusters for management purposes. In the real world, it’s highly recommended to use namespaces, however, you should not think of them as strong security boundaries.\nThe .spec section is where you define the containers that will run in the Pod.\nkubectl apply -f pod.yml →command to POST the manifest to the API server.\nkubectl get pods →command to check the status.\nkubectl get pods hello-pod -o yaml → To get more details about pod\nkubectl describe pods hello-pod →This provides a nicely formatted multi-line overview of an object. It even includes some important object lifecycle events.\nkubectlexec -it hello-pod --sh →command will log-in to the first container\nPod Auto Scalling Types\nCluster (add more nodes when then cluster is full) Horizontal pod autoscale (scale up pod up and down based on metrics) Vertical pod autoscale (increase the resoures limit tool avalible for vertical pod autoscaler) keda (event driven autoscalling) Horizontal Pod Autoscaling (HPA):\napiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: my-app-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: my-app-deployment minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu targetAverageUtilization: 50 ReplicaSet To deploy multiple instance. A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod template.\napiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: guestbook tier: frontend spec: replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v3 Deployments vs ReplicaSet\nDeployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features. Therefore, we recommend using Deployments instead of directly using ReplicaSets\nCMD\nkubectl get rs → get all replica Deployments Deploy Pods indirectly via a higher-level controller. Examples of higher-level controllers include; Deployments, DaemonSets, and StatefulSets.\nFor example, a Deployment is a higher-level Kubernetes object that wraps around a particular Pod and adds features such as scaling, zero-downtime updates, and versioned rollbacks.\nBehind the scenes, Deployments, DaemonSets and StatefulSets implement a controller and a watch loop that is constantly observing the cluster making sure that current state matches desired state.\nDepolyment strategies to manage the rollout and updates of applications within a cluster. These strategies help in achieving continuous delivery, minimizing downtime, and ensuring reliability.\nRolling Update Deployment (default)\nThis strategy gradually replaces old pods with new ones, ensuring that there is always a specified number of replicas available. apiVersion: apps/v1 kind: Deployment metadata: name: example-deployment spec: replicas: 3 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: example spec: containers: - name: example-container image: example:new-version maxUnavailable -\u0026gt; maximum number or percentage of pods that can be unavailable during the update maxSurge -\u0026gt; maximum number or percentage of pods that can be created above the desired replica count during the update Recreate Deployment:\nIn this strategy, the existing pods are terminated all at once, and new pods with the updated version are created. have down time (used on dev ) Blue-Green Deployment:\nthere are two separate environments (blue and green), and the traffic is switched between them after a successful update. Canary Deployment:\ndeployment introduces the new version of the application to a subset of users or traffic. Difference between pod and Deployments Pods don’t self-heal, they don’t scale, and they don’t allow for easy updates or rollbacks. Deployments do all of these. As a result, you’ll almost always deploy Pods via a Deployment controller.\nDesired state is what you want. Current state is what you have. If the two match, everybody’s happy.\nThe declarative model is a way of telling Kubernetes what your desired state is, without having to get into the detail of how to implement it. You leave the how up to Kubernetes.\nReconciliation loops Fundamental to desired state is the concept of background reconciliation loops (a.k.a. control loops). For example, ReplicaSets implement a background reconciliation loop that is constantly checking whether the right number of Pod replicas are present on the cluster. If there aren’t enough, it adds more. If there are too many, it terminates some. To be crystal clear, Kubernetes is constantly making sure that current state matches desired state.\nRolling updates with Deployments Now, assume you’ve experienced a bug, and you need to deploy an updated image that implements a fix. To do this, you update the same Deployment YAML file with the new image version and re-POST it to the API server. This registers a new desired state on the cluster, requesting the same number of Pods, but all running the new version of the image. To make this happen, Kubernetes creates a new ReplicaSet for the Pods with the new image.You now have two ReplicaSets – the original one for the Pods with the old version of the image, and a new one for the Pods with the updated version. Each time Kubernetes increases the number of Pods in the new ReplicaSet (with the new version of the image) it decreases the number of Pods in the old ReplicaSet (with the old versionof the image). Net result, you get a smooth rolling update with zero downtime.\nDeployments manifest files\napiVersion: apps/v1 #Older versions of k8s use apps/v1beta1 kind: Deployment metadata: name: hello-deploy spec: replicas: 10 selector: matchLabels: app: hello-world minReadySeconds: 10 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: hello-world spec: containers: - name: hello-pod image: nigelpoulton/k8sbook:latest ports: - containerPort: 8080 resources: request: (A minimum value) cpu : 1m memory: 1024Mi (in MB) limit: (A maxx value can be used) cpu: 3m memory: The .spec section is where most of the action happens. Anything directly below .spec relates to the Pod. Anything nested below\n.spec.template relates to the Pod template that the Deployment will manage. In this example, the Pod template defines a single container.\n.spec.replicas tells Kubernetes how may Pod replicas to deploy\n.spec.selector is a list of labels that Pods must have in order for the Deployment to manage them.\n.spec.strategy tells Kubernetes how to perform updatesto the Pods managed by the Deployment.Update using the RollingUpdate strategy\nNever have more than one Pod below desired state ( maxUnavailable: 1 ) Never have more than one Pod above desired state ( maxSurge: 1 ) .spec.minReadySeconds This is set to 10 , telling Kubernetes to wait for 10 seconds between each Pod being updated.(on new deployment)\nresources to allocate the min and max resources need for the Pod before make sure that node have enough resources to allocate it by kubctl describe node\nif we not specifiy the resources it take full node resources as it needed.\nkubectl apply -f deploy.yml → will send the yml to API server to deploy\nkubectl get deploy hello-deploy\nYaml file explained API Version specifies the version of the Kubernetes API that should be used for the resource described in that file.The format of the apiVersion field is typically \u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;\nType of groups\nCore API Group (v1): Resources like Pod, Service, Namespace, PersistentVolume, PersistentVolumeClaim, etc. use apiVersion: v1 Apps API Group (apps/v1, apps/v1beta1, apps/v1beta2): for managing higher-level abstractions of applications and workloads, including Deployment, StatefulSet, ReplicaSet, and DaemonSet. Batch API Group (batch/v1, batch/v1beta1): related to batch processing, such as Job and CronJob. Networking API Group (networking.k8s.io/v1, networking.k8s.io/v1beta1): related to networking, including NetworkPolicy. Kind The type of resource being defined.\nPod: Represents a single instance of a running process in a cluster. Service: Defines a set of Pods and a policy to access them as a network service. ReplicationController: Ensures a specified number of replicas for a set of Pods. Deprecated in favor of Deployments and ReplicaSets. Deployment: Declaratively manages a desired state for Pods and ReplicaSets. StatefulSet: Manages the deployment and scaling of a set of Pods with unique identities. DaemonSet: Ensures that a specified Pod runs on each node in the cluster. ReplicaSet: Ensures a specified number of replicas for a set of Pods. Often used by Deployments. Job: Creates one or more Pods and ensures they run to completion. CronJob: Creates Jobs on a schedule specified by a cron expression. Namespace: Provides a way to divide cluster resources into multiple virtual clusters. ConfigMap: Holds configuration data as key-value pairs for Pods to consume. Secret: Stores sensitive information, such as passwords or API keys, as key-value pairs. Ingress: Manages external access to services in a cluster, typically HTTP. NetworkPolicy: Specifies how groups of Pods are allowed to communicate with each other and other network endpoints. ServiceAccount: Provides an identity for processes that run in a Pod. PersistentVolume: Represents a piece of networked storage in the cluster that can be mounted into a Pod. PersistentVolumeClaim: Requests a specific amount of storage from a PersistentVolume. Role and RoleBinding: Define access controls within a namespace. ClusterRole and ClusterRoleBinding: Define access controls across the entire cluster. HorizontalPodAutoscaler: Automatically adjusts the number of Pods in a deployment or replica set. Metadata used to provide data about the resource itself\nname Specifies a name for the resource. The name must be unique within its namespace for most resource types. namespace the namespace in which the resource should be created. labels map of key-value pairs that can be used to organize and categorize resources Spec is used to define the desired state of the resource. The spec section contains the configuration parameters and settings that specify how the resource should behave. The structure of the spec field varies depending on the type of resource being defined, as each resource type has its own set of properties and specifications.\nWhat happend when we apply yml file when we run kubctl apply -f name.yml\nThe kubectl command communicates with the Kubernetes API server.The API server performs several tasks, including authentication, authorization, validation, and admission control. Once the YAML file has passed validation and admission control, the API server persists the resource information in etcd. Controllers in Kubernetes operate on a watch loop. They continuously watch for changes in the desired state of resources by querying etcd for updates.When a new resource is stored in etcd, controllers that are responsible for that resource type are notified. Based on the reconciled desired state, controllers initiate actions to bring the cluster to the desired state. In the case of a Deployment, this may involve creating new Pods to meet the specified replica count. Controller store the Pod object to etcd then the Scheduler is responsible for assigning Pods to available nodes in the cluster.It selects an appropriate node based on factors such as available resources, node affinity/anti-affinity rules, and other constraints specified in the Pod\u0026rsquo;s configuration Next Kubelet get notified for actually creating and managing the containers within Pods Once Kubelet run the pod sucessfully it update the status on etcd. Probe Liveness Probe indicates if the container is operating. If so, no action is taken. If not, the kubelet kills and restarts the container. Readiness Probe indicates whether the application running in the container is ready to accept requests. If so, Services matching the pod are allowed to send traffic to it. If not, the endpoints controller removes the pod from all matching Kubernetes Services.Note:If you don\u0026rsquo;t set the readiness probe, the kubelet assumes that the app is ready to receive traffic as soon as the container starts. If the container takes 2 minutes to start, all the requests to it will fail for those 2 minutes. Startup Probe indicates whether the application running in the container has started. If so, other probes start functioning. If not, the kubelet kills and restarts the container. readinessProbe: httpGet: path: /ready port: 80 Services When newly pods created are scaled it will have new IP so if we have other pod communicating with it. it is unrealible.This is where Services come in to play. Services provide reliable networking for a set of Pods.\nThey operate at the TCP and UDP layer, Services do not possess application intelligence and cannot provide application-layer host and path routing.for that we use ingress\nServices use labels and a label selector to know which set of Pods to load-balance traffic to.\nkube-proxy is responsible for all assign the IP , forwarding packet, load balancing etc.\nTypes ClusterIP (default)\nhas a stable IP address and port that is only accessible from inside the cluster It will get registered in kube DNS and when every Pod IP change kube will take care of it so we can just use the service name to communicate Service YAML\napiVersion: v1 kind: Service metadata: name: hello-svc #Name registered with cluster DNS spec: ports: - port: 8080 #the port the service need to be listen targetPort: 8080 # the port the app is running selector: app: hello-world # Label selector # Service is looking for Pods with the label `app=hello-world` can be any key and vaule deploy.yml apiVersion: apps/v1 kind: Deployment metadata: name: hello-deploy spec: replicas: 10 selector: matchLabels: app: hello-world template: metadata: labels: app: hello-world # Pod labels # The label matches the Service\u0026#39;s label selector spec: containers: - name: hello-ctr the Service has a label selector ( .spec.selector ) with a single value app=hello-world This is the label that the Service is looking for when it queries the cluster for matching Pods.\nThe Deployment specifies a Pod template with the same app=hello-world label ( .spec.template.metadata.labels )\nthe Service will select all 10 Pod replicas and provide them with a stable networking endpoint and load-balance traffic to them.\nkubectl get services → Will list all service with IP get the IP from here\nTo connect with service user the service name or ip by kubectl get services\nRedis(host=\u0026quot;servicename\u0026quot;,port=\u0026quot;8080\u0026quot;) in code level to acess the Pod\nHeadless service:\nSometimes you don\u0026rsquo;t need load-balancing and a single Service IP. In this case, you can create what are termed headless Services, by explicitly specifying \u0026quot;None\u0026quot; for the cluster IP address NodePort service:\nThis builds on top of ClusterIP and enables access from outside of the cluster (static ip). Port range can be only between 30000 to 32767 apiVersion: v1 kind: Service metadata: name: my-service spec: type: NodePort selector: app.kubernetes.io/name: MyApp ports: - port: 80 # By default and for convenience, the `targetPort` is set to # the same value as the `port` field. targetPort: 80 # Optional field # By default and for convenience, the Kubernetes control plane # will allocate a port from a range (default: 30000-32767) nodePort: 30007 Loadbalancer Service\nOn cloud providers which support external load balancers, setting the type field to LoadBalancer provisions a load balancer for your Service. example ingress Ingress An Ingress is an API object that provides HTTP and HTTPS routing to services based on rules defined by the user.\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress spec: rules: - host: myapp.example.com #like ngnix it will accept only the request came form the domain http: paths: - path: /app pathType: Prefix backend: service: name: myapp-service port: number: 80 Ingress Controller is a component responsible for implementing the rules defined in the Ingress resource. It watches for changes to Ingress resources and configures the underlying load balancer or reverse proxy to handle incoming requests accordingly. Popular Ingress Controllers include NGINX Ingress Controller, Traefik, and HAProxy Ingress\nEndpoint Objects Kubernetes is constantly evaluating the Service’s label selector against the current list of healthy Pods on the cluster. Any new Pods that match the selector get added to the Endpoints object, and any Pods that disappear get removed. This means the Endpoints object is always up to date. Then, when a Service is sending traffic to Pods, it queries its Endpoints object for the latest list of healthy matching Pods. Every Service gets its own Endpoints object with the same name as the Service.This object holds a list of all the Pods the Service matches and is dynamically updated as matching Pods come and go.\nCMD\nkubectl get pod -o wide → will display the IP kubectl apply -f svc.yml →Tells Kubernetes to deploy a new object from a file called svc.yml . The .kind field in the YAML file tells Kubernetes that you’re deploying a new Service object. kubectl get svc hello-svc →Introspecting Services display the IP kubectl get ep hello-svc →the Endpoint controller’s Kubernetes DNS Every Pod on the cluster, meaning all containers and Pods know how to find it. Every new service is automatically registered with the cluster’s DNS so that all components in the cluster can find every Service by name. Some other components that are registered with the cluster DNS are StatefulSets and the individual Pods that a StatefulSet manages.Cluster DNS is based on CoreDNS (https://coredns.io/).\nKubernetes storage When we restart the pod the data stored in pod will go. To solve this we using Kubernetes storage\nStorage for single pod\napiVersion: apps/v1 kind: Deployment metadata: name: local-web spec: replicas: 1 selector: matchLabels: app: local-web template: metadata: labels: app: local-web spec: containers: - name: local-web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: local -\u0026gt; label must need to match with voulme mountPath: /usr/share/nginx/html #Path inside the container volumes: - name: local hostPath: path: /var/nginxserver When we store data in /usr/share/nginx/html in this dir it will be stored in /var/nginxserver this kind of storage cannot be shared among the pod and nodes Shared storage\nThe three main resources in the persistent volume subsystem are:\nPersistent Volumes (PV) Storage Classes (SC) → Type of storage class ex NFS,AWS,azure..etc Persistent Volume Claims (PVC) → used to claim the storage that was alloacted using PV First create PV with following\napiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: capacity: storage: 500Mi persistentVolumeReclaimPolicy: Retain accessModes: - ReadWriteMany storageClassName: nfs #type of storage that we using nfs: server: 192.168.1.7 path: \u0026#34;/srv/nfs\u0026#34; .spec.accessModes defines how the PV can be mounted. Three options exist: ReadWriteOnce (RWO) ReadWriteMany (RWM) ReadOnlyMany(ROM) ReadWriteOnce defines a PV that can only be mounted/bound as R/W by a single PVC. Attempts from multiple PVCs to bind (claim) it will fail. ReadWriteMany defines a PV that can be bound as R/W by multiple PVCs. This mode is usually only supported by file and object storage such as NFS. Block storage usually only supports RWO . ReadOnlyMany defines a PV that can be bound by multiple PVCs as R/O. A couple of things are worth noting. First up, a PV can only be opened persistentVolumeReclaimPolicy This tells Kubernetes what to do with a PV when its PVC has been released. Two policies currently exist: Delete →This policy deletes the PV and associated storage resource on the external storage system Retain Deploy the PV by kubctl -f apply filename.yml\nkubctl get pv : to get all PV PV are not bound to namespace it can be accesed by any namspace Create PVC to claim the pV\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: mycustomvolume spec: accessModes: - ReadWriteMany storageClassName: nfs #we can create custom storage class resources: requests: storage: 100Mi4 .spec section must match with the PV you are binding it with. In this example, access modes, storage class, and capacity must match with the PV. but the storage can be less then what we give in PV It bound to namespace kubctl describe pvc → to describe what’ going on helpfull to debug. Use it in deployment\napiVersion: apps/v1 kind: Deployment metadata: name: nfs-web spec: replicas: 1 selector: matchLabels: app: nfs-web template: metadata: labels: app: nfs-web spec: containers: - name: nfs-web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: nfs mountPath: /usr/share/nginx/html volumes: - name: mycustomvolume #Label need to match persistentVolumeClaim: claimName: nfs Config Maps Kubernetes provides an object called a ConfigMap (CM) that lets you store configuration data outside of a Pod. It also lets you dynamically inject the data into a Pod at run-time. ConfigMaps are a map of key/value pairs and we call each key/value pair an entry.\nExample\n#multimap.yml kind: ConfigMap apiVersion: v1 metadata: name: multimap data: DB_URL: \u0026#34;\u0026#34; # this can be acessed as var .env : | # these content stored in file name called .env on pod env = plex-test endpoint = 0.0.0.0:31001 char = utf8 vault = PLEX/test log-size = 512M kubectl apply -f multimap.yml Use Pipe (|) to create a as .env file Injecting ConfigMap data into Pods and containers\napiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: registry.k8s.io/busybox command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] env: # Define the environment variable - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY name: multimap # Specify the key associated with the value key: DB_URL ConfigMaps and volumes\nUsing ConfigMaps with volumes is the most flexible option. You can reference entire configuration files as well as make updates to the ConfigMap and have them reflected in running containers.\nCreate the ConfigMap Create a ConfigMap volume in the Pod template Mount the ConfigMap volume into the container Entries in the ConfigMap will appear in the container as individual files Secretes Secrets can be created independently of the Pods that use them, there is less risk of the Secret (and its data) being exposed during the workflow of creating, viewing, and editing Pods. Kubernetes, and applications that run in your cluster, can also take additional precautions with Secrets, such as avoiding writing sensitive data to nonvolatile storage.\nKubernetes Secrets are, by default, stored unencrypted in the API server\u0026rsquo;s underlying data store (etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment.\napiVersion: v1 kind: Secret metadata: name: dotfile-secret data: .secret-file: dmFsdWUtMg0KDQo= --- apiVersion: v1 kind: Pod metadata: name: secret-dotfiles-pod spec: volumes: - name: secret-volume secret: secretName: dotfile-secret containers: - name: dotfile-test-container image: registry.k8s.io/busybox command: - ls - \u0026#34;-l\u0026#34; - \u0026#34;/etc/secret-volume\u0026#34; volumeMounts: - name: secret-volume readOnly: true mountPath: \u0026#34;/etc/secret-volume\u0026#34; StatefulSets StatefulSets in Kubernetes are a resource type used to manage stateful applications, particularly those that require unique identities, stable network identifiers, and stable persistent storage. They are used for applications like databases where each instance needs a specific identity and state.\nUsing StatefulSets for MongoDB:\nPod Identity: StatefulSets assign stable, predictable identities (like mongo-0, mongo-1, etc.) to each MongoDB instance. This ensures consistency and stability crucial for proper replication and cluster coordination. Storage: StatefulSets facilitate the use of persistent volumes, ensuring that each MongoDB instance has its dedicated and persistent storage. Even if a pod fails or needs to be rescheduled, the data remains intact and can be reattached to a new pod with the same identity. StatefulSet Pod naming\nAll Pods managed by a StatefulSet get predictable and persistent names. These names are vital, and are at the core of how Pods are started, self-healed, scaled, deleted, attached to volumes, and more. The format of StatefulSet Pod names is StatefulSetName-Integer . The integer is a zero-based index ordinal, which is just a fancy way of saying “number starting from 0”\nOrdered creation and deletion\nStatefulSets create one Pod at a time, and always wait for previous Pods to be running and ready before creating the next. This is different from Deployments that use a ReplicaSet controller to start all Pods at the same time,causing potential race conditions.\nDeleting StatefulSets\nFirstly, deleting a StatefulSet does not terminate Pods in order. With this in mind, you may want to scale a StatefulSet to 0 replicas before deleting it. You can also use terminationGracePeriodSeconds to further control the way Pods are terminated. It’s common to set this to at least 10 seconds to give applications running in Pods a chance to flush local buffers and safely commit any writes that are still in-flight.\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx\u0026#34; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry.k8s.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi More Example refer here\nAutoScale Kubectl Kubectl is the main Kubernetes command-line tool and is what you will use for your day-to-day Kubernetes management activities.\nkubectl configuration file is called config and lives in $HOME/.kube . It contains definitions for: • Clusters • Users (credentials) • Contexts\nClusters is a list of clusters that kubectl knows about and is ideal if you plan on using a single workstation to manage multiple clusters. Each cluster definition has a name, certificate info, and API server endpoint.\nUsers let you define different users that might have different levels of permissions on each cluster. For example, you might have a dev user and an ops user, each with different permissions. Each user definition has a friendly name, a username, and a set of credentials.\nContexts bring together clusters and users under a friendly name. For example, you might have a context called deploy-prod that combines the deploy user credentials with the prod cluster definition. If you use kubectl with this context you will be POSTing commands to the API server of the prod cluster as the deploy user.\nkubctl version -\u0026gt; to get version\nkube config file\napiVersion: v1 clusters: - cluster: certificate-authority-data: \u0026#34;\u0026#34; server: https:/localhost:443 ## master node name: test contexts: - context: cluster: test user: clusterUser_test_aks_rg_test name: test current-context: test ## Curren kind: Config preferences: {} users: - name: clusterUser_test_aks_rg_test user: client-certificate-data: \u0026#39;\u0026#39; client-key-data: \u0026#39;\u0026#39; token: \u0026#39;\u0026#39; kubectl config current-context- tell which context we are accessing currently now kubectl config get-context -\u0026gt; to get all context kubectl config use-context context-name -\u0026gt; to switch the context to different cluster Namespaces Used to isolate the resources by default it have default namespace\nkubctl get namespace kubctl -n nampace_name cmd\nkubectl logs podname\nkubctl exec it podname\nbest practices Preventing broken connections during pod shutdown When a request for a pod deletion is received by the API server, it first modifies the state in etcd and then notifies its watchers of the deletion. Among those watchers are the Kubelet and the Endpoints controller.the pod being removed from the iptables rules when the Endpoints controller recevie notification pod is deleted.\nDuring any client trying to connect to it receives a Connection refused error. to solve this do following\nGraceful Termination in Application Code Modify the application code to handle termination signals gracefully. When a container in a pod receives a SIGTERM signal, it\u0026rsquo;s an indication that the pod is about to be terminated. During the shutdown process, your application should stop accepting new requests and finish processing existing ones Pre-Stop Hook Kubernetes supports a pre-stop hook, which is a user-defined script that is executed before a container is terminated. Pod Termination Grace Period specify a termination grace period (terminationGracePeriodSeconds: 30) for a pod, which is the amount of time the system waits for your application to shut down gracefully. During this period, the pod remains in the \u0026ldquo;Terminating\u0026rdquo; state, allowing your application to complete any remaining work Readiness Probes: can be used to prevent new requests from being directed to a pod Service Load Balancing: Need to study\njob cronjob operator Resources Give a idea about and overview of the architecture of that Design principle https://dev.to/leandronsp/kubernetes-101-part-i-the-fundamentals-23a1 kubernetes 101 series https://github.com/jatrost/awesome-kubernetes-threat-detection https://openai.com/research/scaling-kubernetes-to-7500-nodes https://github.blog/2019-11-21-debugging-network-stalls-on-kubernetes/ https://iximiuz.ck.page/posts/ivan-on-containers-kubernetes-and-backend-development https://dev.to/therubberduckiee/explaining-kubernetes-to-my-uber-driver-4f60 kubernetes -o architecture https://towardsdev.com/understanding-control-pane-and-nodes-in-k8s-architecture-5572018f7624 Demystifying container networking Handling Client Requests Properly with Kubernetes production-best-practices Internal Resources Collection of resources for inner workings of Kubernetes Products To cut down the cost of kubernetes The first cloud native service provider powered only by Kubernetes ","permalink":"https://programmerraja.github.io/blog/notes/2024/kubernetes/","summary":"Kubernetes is two things A cluster for running applications An orchestrator of cloud-native microservices apps Kubernetes cluster A Kubernetes cluster contains six main components:\nAPI server: Exposes a REST interface to all Kubernetes resources. Serves as the front end of the Kubernetes control plane. Scheduler: Places containers according to resource requirements and metrics. Makes note of Pods with no assigned node, and selects nodes for them to run on. Controller manager: Runs controller processes and reconciles the cluster’s actual state with its desired specifications.","title":"Kubernetes"},{"content":"How LinkedIn Adopted Protocol Buffers to Reduce Latency by 60% They heavly using microservice to avoid the latency between the service they go with protocol buffers Protocol buffer (Protobuf) is a data serialization format and a set of tools to exchange data. Protobuf keeps data and the metadata separate. And serializes data into binary format. Use Protobuf when: Payload is big Communication between non-JavaScript environments needed Frequent changes to the payload schema expected Scaling Slack’s Job Queue They using redis queue for all async operation they have face few issue with that such as Operational Memory Constraints in Redis, Complex Redis Connection Structure ,Worker Scalability Dependency on Redis etc Instead fully replace with kafa they go step by step.First they push message to kafa and from kafa they have written JQRelay which is a stateless service written in Go that relays jobs from a Kafka topic to its corresponding Redis cluster. When a JQRelay instance starts up, it attempts to acquire a Consul lock on an key/value entry corresponding to the Kafka topic. If it gets the lock, it starts relaying jobs from all partitions of this topic. Hashnode\u0026rsquo;s Overall Architecture They caching mechanism page cache using next js API cache using GraphQL Edge Caching with Stellate and Serverless Function Caching with Vercel Scaling Cron Monitoring They have Relay which listen for check in and other event from cron and push to kafa\nSentry Django application will consume from and do the heavy lifting of storing and processing those check-ins\nTo inform the user about missed check in they written a job which run for a minute and checks on table whether it has check in if it not inform the user but it have 2 problem\nAssume due to overload in kafa consumer get consumed later but the job runs before and assume it has not checked in During deploys of the celery beat scheduler, there is a short period where tasks may be skipped. Their solution involved leveraging the timestamp of each check-in message to determine when all check-ins up to a minute boundary had been processed. They tracked the last time a minute boundary was crossed based on these timestamps. Once they reached a new minute boundary, they knew all check-ins before that moment had been consumed.\nFor instance, imagine a sequence of check-in messages with timestamps:\nMessage 1: Timestamp 12:03:20 Message 2: Timestamp 12:03:45 Message 3: Timestamp 12:04:02 When Message 3 arrives, it crosses the minute boundary of 12:04. This signals that all check-ins before 12:04 have been processed. This event becomes the trigger to generate tasks for detecting missing check-ins, eliminating the need for periodic celery beat tasks. This way, they avoid skipping tasks during deployment and accurately detect missed check-ins even during Kafka backlog scenarios.\nAn overview of Cloudflare\u0026rsquo;s logging pipeline They used BSD syslog protocol to send the log from system\nSyslog-ng is a daemon that implements the aforementioned BSD syslog protocol. In our case, it reads logs from journald, and applies another layer of rate limiting. It then applies rewriting rules to add common fields, such as the name of the machine that emitted the log, the name of the data center the machine is in\nLogs are pushed to Kafka\nLogs stored in ELK stack, and a Clickhouse cluster\nElasticSearch with 90 cluster Clickhouse with 10 Nodes of cluster ","permalink":"https://programmerraja.github.io/blog/notes/2024/learning-from-blog/","summary":"How LinkedIn Adopted Protocol Buffers to Reduce Latency by 60% They heavly using microservice to avoid the latency between the service they go with protocol buffers Protocol buffer (Protobuf) is a data serialization format and a set of tools to exchange data. Protobuf keeps data and the metadata separate. And serializes data into binary format. Use Protobuf when: Payload is big Communication between non-JavaScript environments needed Frequent changes to the payload schema expected Scaling Slack’s Job Queue They using redis queue for all async operation they have face few issue with that such as Operational Memory Constraints in Redis, Complex Redis Connection Structure ,Worker Scalability Dependency on Redis etc Instead fully replace with kafa they go step by step.","title":"Learning from blog"},{"content":"What is RabbitMQ? RabbitMQ is a popular open-source message broker that implements the AMQP (Advanced Message Queuing Protocol) standard.\nChannel A channel is a separate communication channel within a single connection. Channels allow multiple concurrent exchanges to be executed in parallel, providing a way to separate different parts of your application.\nEach channel has its own set of resources, such as queues, exchanges, and bindings, which are independent of the resources used by other channels in the same connection. This allows you to manage concurrency and improve performance by isolating different parts of your application into separate channels.\nFor example, you might create one channel for sending messages and another channel for receiving messages, or create separate channels for different types of messages or different parts of your application. By using multiple channels, you can take advantage of the scalability and performance benefits of AMQP while also making it easier to manage your application.\nChannel vs connection In RabbitMQ, a connection represents a network connection to the RabbitMQ broker. When a client connects to RabbitMQ, it establishes a TCP connection to the broker. This connection remains open until the client explicitly closes it or the broker closes it due to a network error or a timeout. A connection is created using the AMQP protocol, and it is responsible for authentication, connection handling, and connection-level flow control.\nOn the other hand, a channel is a virtual connection inside a connection that allows multiple logical connections to be multiplexed over a single physical connection. When a client establishes a connection to RabbitMQ, it can create one or more channels inside that connection. Each channel is a separate AMQP session that can be used to publish or consume messages, declare queues and exchanges, and bind queues to exchanges.\nThe main difference between a connection and a channel is that a connection represents a physical connection to the broker, whereas a channel represents a logical connection within that physical connection. Channels allow multiple AMQP operations to be multiplexed over a single network connection, which can help reduce the overhead of establishing multiple network connections.\nIn summary, a connection in RabbitMQ represents a physical network connection to the broker, while a channel represents a logical connection within that physical connection, allowing multiple AMQP operations to be multiplexed over a single network connection.\nExchanges In RabbitMQ, an exchange is a message routing agent that receives messages from producers and routes them to queues based on message properties such as the routing key. When a producer sends a message to an exchange, it is up to the exchange to route the message to one or more queues.\nThere are four types of exchanges in RabbitMQ:\nDirect exchange: Messages are routed to queues based on the exact match between the routing key of the message and the routing key of the queue.\nFanout exchange: Messages are routed to all the queues bound to the exchange. It ignores the routing key and sends messages to all the queues that are bound to the exchange.\nTopic exchange: Messages are routed to queues based on pattern matching between the routing key of the message and the routing key of the queue. It uses wildcards to match the routing key.\nHeaders exchange: Messages are routed to queues based on header values instead of routing keys. It is rarely used, and its functionality is similar to the topic exchange.\nx-delayed-message → is a custom exchange type in RabbitMQ that allows you to delay messages before they are delivered to a queue. This exchange type is not included in RabbitMQ by default and must be installed as a plugin.\nWhen you create an x-delayed-message exchange, you can set a delay time for messages using the x-delay header. The exchange will hold the message for the specified delay time and then deliver it to the appropriate queue. This is useful in scenarios where you want to delay the delivery of a message until a certain time or after a certain event has occurred.\nExample\nconst amqp = require(\u0026#39;amqplib\u0026#39;); async function setup() { // Connect to RabbitMQ const connection = await amqp.connect(\u0026#39;amqp://localhost\u0026#39;); const channel = await connection.createChannel(); // Install the delayed message exchange plugin (if necessary) await channel.assertExchange(\u0026#39;amq.delayed\u0026#39;, \u0026#39;x-delayed-message\u0026#39;, { durable: true, arguments: { \u0026#39;x-delayed-type\u0026#39;: \u0026#39;direct\u0026#39; } }); // Create the queue and bind it to the delayed message exchange const queueName = \u0026#39;my-queue\u0026#39;; const routingKey = \u0026#39;my-routing-key\u0026#39;; await channel.assertQueue(queueName, { durable: true }); await channel.bindQueue(queueName, \u0026#39;amq.delayed\u0026#39;, routingKey); // Consumer function to handle incoming messages const consumerFunction = (msg) =\u0026gt; { console.log(`Received message: ${msg.content.toString()}`); channel.ack(msg); }; // Consume messages from the queue channel.consume(queueName, consumerFunction); // Producer function to send messages with a delay const producerFunction = async (message, delayMs) =\u0026gt; { const headers = { \u0026#39;x-delay\u0026#39;: delayMs }; const buffer = Buffer.from(message); await channel.publish(\u0026#39;amq.delayed\u0026#39;, routingKey, buffer, { headers }); console.log(`Sent message \u0026#34;${message}\u0026#34; with delay of ${delayMs}ms`); }; // Send some messages with a delay await producerFunction(\u0026#39;Hello World!\u0026#39;, 5000); await producerFunction(\u0026#39;Delayed message!\u0026#39;, 10000); } setup(); Application of Exchange\nLet\u0026rsquo;s say you have a distributed system that consists of multiple microservices. Each microservice has its own queue, and they communicate with each other through messaging.\nWhen a microservice wants to send a message to another microservice, it can publish the message to an exchange instead of directly sending it to the other microservice\u0026rsquo;s queue. The exchange is responsible for routing the message to the appropriate queue based on the message\u0026rsquo;s routing key.\nFor example, let\u0026rsquo;s say you have a microservice that handles user authentication, and another microservice that handles user orders. When a user logs in, the authentication microservice can publish a message to the exchange with a routing key of \u0026ldquo;user.login\u0026rdquo;. The exchange can then route this message to the order microservice\u0026rsquo;s queue, which is bound to the exchange with a matching routing key.\nIf you didn\u0026rsquo;t use an exchange, the authentication microservice would have to know the exact name of the order microservice\u0026rsquo;s queue and send the message directly to that queue. This would tightly couple the microservices and make it harder to make changes to the system in the future.\nBy using an exchange, you can create a loosely coupled system where microservices don\u0026rsquo;t need to know about each other\u0026rsquo;s queues. This makes it easier to add new microservices or change the routing rules of messages without affecting other parts of the system.\nDurablity RabbitMQ provides durability by persisting messages and metadata to disk. This ensures that messages are not lost in case of a server failure.if it durable to true it will presist if we set false it will not until it reach threshold\nPrefetch RabbitMQ uses prefetch to control the amount of messages a consumer can consume at once. Prefetch specifies the number of unacknowledged messages that can be in-flight before the broker stops delivering more messages to the consumer. This avoids overloading a consumer with too many messages at once.\nAmqp-connection-manager vs Amqplib amqp-connection-manager and amqplib are both Node.js libraries for working with RabbitMQ, but they have different purposes and use cases.\namqplib is a low-level RabbitMQ client library that provides a thin wrapper around the RabbitMQ API. It allows you to send and receive messages, create and manage exchanges and queues, and interact with other RabbitMQ features. amqplib provides a direct and flexible interface to RabbitMQ, and is a good choice if you need complete control over your RabbitMQ interactions.\namqp-connection-manager, on the other hand, is a higher-level library that provides connection management and channel pooling. It uses amqplib under the hood, but adds features like connection retry, connection throttling, and automatic channel recovery. amqp-connection-manager is a good choice if you want to simplify your RabbitMQ code and reduce the chance of connection errors, or if you need to handle multiple connections and channels.\nIn general, if you need fine-grained control over your RabbitMQ interactions, or if you have a small number of connections and channels, amqplib is a good choice. If you have a large number of connections and channels, or if you want to simplify your RabbitMQ code and reduce the chance of connection errors, amqp-connection-manager is a better choice.\nWhat are the best practice regarding channel and connection Use a single connection per application instance: It\u0026rsquo;s a good practice to use a single connection for an entire application instance. Creating multiple connections can lead to resource wastage, and can make it difficult to manage and monitor connections. Use a connection pool: Creating and closing connections can be expensive, so it\u0026rsquo;s recommended to use a connection pool. Connection pools can be used to manage the number of connections and can help improve performance. Use a separate channel for each thread: When creating a multithreaded application, use a separate channel for each thread instead of sharing a single channel. Sharing a single channel between threads can lead to contention issues and can cause the application to become unstable. Close channels when they\u0026rsquo;re no longer needed: It\u0026rsquo;s a good practice to close channels when they\u0026rsquo;re no longer needed. This helps to reduce the number of open channels and frees up resources. Use a lightweight protocol: RabbitMQ provides AMQP, which is a lightweight protocol that is designed for message queuing. Using a lightweight protocol can help improve performance and reduce the overhead of managing connections and channels. Use transactional channels: When sending multiple messages, it\u0026rsquo;s a good practice to use transactional channels. This ensures that all messages are either sent successfully or not sent at all. Use a connection heartbeat: RabbitMQ provides a connection heartbeat mechanism that can be used to detect network failures. It\u0026rsquo;s a good practice to use connection heartbeat to ensure that the application can recover from network failures. Use connection and channel events: RabbitMQ provides connection and channel events that can be used to monitor and manage connections and channels. Using these events can help improve the reliability and performance of the application. How to Handle failures If we want to retry if the consumer get failed when processing the data we can have 2 way 1. nack 2. reject\nDifference between them\nThe main difference between nack (negative acknowledgement) and reject in RabbitMQ is how they handle message rejection and requeuing:\nnack (channel.nack):\nnack is used to negatively acknowledge a message and reject it. It allows you to control whether the message should be requeued or discarded. The method signature is channel.nack(message, allUpTo, requeue). The message parameter represents the message being rejected. The allUpTo parameter is a boolean that indicates whether all unacknowledged messages prior to the given message should also be rejected. The requeue parameter is a boolean that determines whether the rejected message should be requeued or discarded. With channel.nack, you have more control over requeuing behavior and can choose whether to discard the message or requeue it for retry. reject (channel.reject):\nreject is used to reject a message without acknowledging it. When a message is rejected using reject, it can optionally be requeued based on the requeue parameter. The method signature is channel.reject(message, requeue). The message parameter represents the message being rejected. The requeue parameter is a boolean that determines whether the rejected message should be requeued or discarded. By default, if requeue is set to true, the message will be requeued for future delivery. If set to false, the message will be discarded. In summary, nack provides more flexibility by allowing you to explicitly control requeuing behavior (requeue or discard), whereas reject gives you the option to requeue the message based on the requeue parameter. Both methods can be used to handle message rejection and retry scenarios in RabbitMQ, depending on your specific requirements.\nRabbitmq simulator tool to playaround with it\nInternal Protocol : AMQP\nBlog\nhow does elrang does scheduling Resources https://www.cloudamqp.com/blog/part1-rabbitmq-best-practice.html https://www.cloudamqp.com/blog/part2-rabbitmq-best-practice-for-high-performance.html What we learned form running 1k queue Cloudamp Keep connection and channle as low as possible seprate connection for pub and consume4 dont have too large queue 10k msg Enable lazy queue (loaded on ram when needed) rabbitmq sharding limited use on priroity queue adjust prefetch (cloudampq default 1k ) Explain basic and how to handle error case https://www.cloudamqp.com/blog/when-to-use-rabbitmq-or-apache-kafka.html https://youtu.be/HzPOQsMWrGQ ","permalink":"https://programmerraja.github.io/blog/notes/2023/rabbitmq/","summary":"What is RabbitMQ? RabbitMQ is a popular open-source message broker that implements the AMQP (Advanced Message Queuing Protocol) standard.\nChannel A channel is a separate communication channel within a single connection. Channels allow multiple concurrent exchanges to be executed in parallel, providing a way to separate different parts of your application.\nEach channel has its own set of resources, such as queues, exchanges, and bindings, which are independent of the resources used by other channels in the same connection.","title":"RabbitMQ Notes"},{"content":"Hi, I\u0026rsquo;m K.Boopathi! 👋 About Me I\u0026rsquo;m a passionate programmer with an insatiable curiosity for exploring diverse technologies and leveraging them to create cutting-edge products. My journey in the realm of tech is driven by a fervent desire to learn and innovate.\nWhat I Do My blog is a reflection of my love for web development, offering insightful content, tutorials, and invaluable resources. I\u0026rsquo;m dedicated to sharing my knowledge and experiences, empowering aspiring developers to embark on their own coding adventures.\nWhy I Write With each blog post, I aim to bridge the gap between complex concepts and practical application, fostering a community of learners eager to excel in the ever-evolving world of technology.\nLet\u0026rsquo;s Connect Join me in this exciting tech journey! Dive into my blogs, explore new horizons in web development, and let\u0026rsquo;s innovate together! 🌟✨\n","permalink":"https://programmerraja.github.io/blog/post/2023/helloworld/","summary":"Hi, I\u0026rsquo;m K.Boopathi! 👋 About Me I\u0026rsquo;m a passionate programmer with an insatiable curiosity for exploring diverse technologies and leveraging them to create cutting-edge products. My journey in the realm of tech is driven by a fervent desire to learn and innovate.\nWhat I Do My blog is a reflection of my love for web development, offering insightful content, tutorials, and invaluable resources. I\u0026rsquo;m dedicated to sharing my knowledge and experiences, empowering aspiring developers to embark on their own coding adventures.","title":"Hello world"},{"content":"MongoDB Architecture and Concepts if the write concern is set to “majority”, then the database will not complete the write operation until a majority of secondaries receive the write. We can also set the write concern to wait until all secondaries or a specific number of secondaries receive the write operation.\nOplog The primary node stores information about document changes in a collection within its local database called the Oplog. The primary will continuously attempt to apply these changes to secondary instances.\nTools of the Trade In order to obtain the execution statistics, explain(\u0026ldquo;executionStats\u0026rdquo;) will fully execute the MongoDB statement concerned. This means that it may take much longer to complete than a simple explain() and place significant load on the MongoDB server.\nExcutionStats Takes more time compare to explain().\nProfiling Profiling information in MongoDB is stored within the system.profile collection. This collection follows a circular structure, meaning it has a fixed size. When this size is surpassed, older entries are automatically removed to accommodate new ones. The default size for system.profile is set to 1MB, but it might be advisable to consider increasing it based on your needs. To achieve this, one can halt profiling, drop the existing collection, and recreate it with an expanded size.\nAdjusting the profiling level is facilitated through the db.setProfilingLevel(level, {slowms: slowMsThreshold, sampleRate: samplingRate}) command. The level parameter can be set to:\n- 0 to disable profiling. - 1 to collect only queries surpassing a specified time threshold (`slowms`). - 2 to collect all queries indiscriminately. The samplingRate parameter, which determines the random sampling level, is a crucial aspect. For instance, if set to 0.5, approximately half of all statements will undergo profiling, providing a representative sample.\nThis feature is instrumental in managing the trade-off between profiling accuracy and performance overhead. Adjusting these parameters enables a fine-tuned control over the profiling process, catering to specific optimization and analysis requirements.\nServer Status The db.serverStatus() command is a quick and powerful way to get a lot of high-level information about your MongoDB server DOC will return all server detail example how read and updated happens and so many thing how many connections are active\nHowever, there are two problems with using db.serverStatus() in this way. Firstly, these counters don’t tell us much about what’s happening on the server right now, making it difficult to identify which of the metrics may be impacting the performance of our application\nExamining Current Operations Another useful tool when tuning performance in MongoDB is the db.currentOp() command. This command works as you might imagine – it returns information about operations that are currently running on the database\nIndexing Index scans are not always a good thing. If the range is extensive, then the index scan might be worse than not using an index at all.\nIndex is not suitable if we going to read all docs but mongo use index u can specify to use col scan hint({$natural:1}),\nTo fully optimize an $or query, index all the attributes in the $or array.\nAn $exists:true lookup can be optimized by a sparse index on the attribute concerned. However, such an index cannot optimize an $exists:false query.\nIf you want to do case-insensitive searches, then there is a trick you can use. First, create an index with a case-insensitive collation sequence. This is done by specifying a collation sequence with a strength db.customers.createIndex({ LastName: 1 },{ collation: { locale: 'en', strength: 2 } });\nIndex Merges if we have 2 index one for a and b if search of a and b mongodb will merges the index value on search\n1 IXSCAN a_1 2 IXSCAN b_1 3 AND_SORTED 4 FETCH The AND_SORTED step indicates that an index intersection has been performed. Index intersections for $and conditions are unusual\nType of index Compound: A compound index is simply an index comprising more than one attribute. The most significant advantage of a compound index is that it is usually more selective than a single key index. The combination of multiple attributes will point to a smaller number of documents than indexes composed of singular attributes. A compound index that contains all of the attributes contained within the find() or $match clauses will be particularly effective createIndex({key1:1,key2:1})\nPartial: A partial index is one which is only maintained for a subset of information createIndex({key1:{$exists:true}})\nSparse: sparse index doesn’t include documents that don’t contain the indexed attributes. a sparse index cannot support an $exists:true search on the indexed attribute: By contrast, non-sparse indexes contain all documents in a collection, storing null values for those documents that do not contain the indexed field. createIndex( { \u0026quot;key\u0026quot;: 1 }, { sparse: true } )\nGeospatial Indexes: Typically need to perform searches across map data\nText Index: MongoDB uses a method called suffix stemming to build a search index. Suffix stemming involves finding a common element (prefix) at the start of each word which forms the root of a search tree. Each divergent suffix “stems” off into its own node that may stem further. This process creates a tree that can be efficiently searched from the root (the most common shared element) down to the leaf node, with the path from the root to the leaf node forming a complete word.\nFor example, suppose we have the words “finder,” “finding,” and “findable” somewhere in our documents. By using suffix stemming, we could find a common root in these words of “find,” and then the suffixes stemming from this term would be “er,” “ing,” and “able.” createIndex({description: \u0026quot;text\u0026quot;})\nFinding Unused Indexes we can take a look at index utilization by using the $indexStats aggregation command:\n{ \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;key\u0026#34;: 1 }, \u0026#34;host\u0026#34;: \u0026#34;xxx:27017\u0026#34;, \u0026#34;accesses\u0026#34;: { \u0026#34;ops\u0026#34;: NumberLong(145), \u0026#34;since\u0026#34;: ISODate(\u0026#34;2017-11-25T16:21:36.285Z\u0026#34;) } } OPS :This is the number of operations that used the index Since : This is the time from which MongoDB gathered stats, and is typically the last start time. Query Tuning Optimizing Network Round Trips Projections Batch Processing Bulk Inserts Batch Processing\nWhen retrieving data using a cursor, you can specify the number of rows fetched in each operation using the batchSize clause. For instance, in the following we have a cursor where the variable batchSize controls the number of documents retrieved from the MongoDB database in each network request:\ndb.millions.find({},{n:1,_id:0}).batchSize(batchsize); Overriding the Optimizer with Hints We can change force this query to use a collection scan by adding a hint. If we append .hint({$natural:1}), we instruct MongoDB to perform a collection scan to resolve the query:\nOptimizing Sort Operations Blocking sort\nThe sorting that doing without using any index is called blocking sort because the it need fetch all doc to memory and need to do sorting and return the doc to the user\nif you are going to to do a blocking sort on a large amount of data, you may need to allocate more memory for the sort. You can do this by adjusting the internal parameter internalQueryExecMaxBlockingSortBytes.\ndb.getSiblingDB(\u0026#34;admin\u0026#34;).runCommand( { setParameter: 1, internalQueryExecMaxBlockingSortBytes:1001048576 }); Hint: Beware of index-supported $ne queries. They resolve to multiple index range scans which might be less effective than a collection scan.\nTuning AggregationPipelines The method mongoTuning.aggregationExecutionStats() will provide a top-level summary of the time taken byeach step.\nvar exp = db.customers.explain(\u0026#39;executionStats\u0026#39;).aggregate(query) mongoTuning.aggregationExecutionStats(exp) Optimizing Aggregation Ordering MongoDB will automatically resequence the order of operations in a pipeline to optimize performance. one such case where automatic reordering is not possible is aggregations using $lookup. The $lookup stage allows you to join two collections\nAutomatic Pipeline Optimizations $sort and $limit stages will be merged, allowing the $sort to only maintain the limited number of documents instead of its entire input. If your aggregation only requires a subset of document attributes,MongoDB may add a projection to remove all unused fields.when we use group mongodb will add projection before the group stage Indexed Aggregation Sorts $sort can be utilized the index only if it in early enough the pipeline to be rolled into the initial data access operation. aggregate([ { $sort:{ d:1 }}, {$addFields:{x:0}} ], {allowDiskUse: true} ); The above query will use the index but if we swap the order it won\u0026rsquo;t use index because When MongoDB executes $addFields, it applies the specified expression to each document in the aggregation pipeline, introducing a modification to the document structure. This modification, in turn, may invalidate certain optimization opportunities associated with the index. When we first add the fields to the doc and if we do sorting we cannot use index because the when we index lookup and sort it we again need lookup on disk to get the document which will not have the newly added field. Inserts, Updates, and Deletes Indexes always add to the overhead of insert and delete statements and may add to the overhead of update statements. Avoid over-indexing, especially on columns which are frequently updated.\nWrite Concern (waiting for replicas to write/update happen): Adjusting writeConcern can improve performance, but it may come at the expense of data integrity or safety. Don\u0026rsquo;t adjust writeConcern to improve performance unless you are fully aware of these trade-offs.\nMongoDB 4.2, we have the ability to embed aggregation framework pipelines within an update statement. These pipelines allow us to set a value that is derived from, or dependent on, other values in the document. For instance, we could populate the viewCount attribute with this single query\ndb.collection.update({},[{ $set: { viewCount: { $size: \u0026#39;$views\u0026#39; } } }]) Server Monitoring Host-Level Monitoring Top, uptime,vmstat,netstat,etc.. Network bwn-ng -\u0026gt; You can monitor the amount of data transferring traceroute mongors03.koreacentral.cloudapp.azure.com \u0026ndash;port=27017 -T -\u0026gt; to find how many miilisec it take to reach make sure it is less then 100ms Memory vmstat -s : print active, inactive ,swap memory etc.. iostat : for I/O Memory Tuning MongoDB memory architecture It uses WiredTiger storage engine.Each connection uses up to 1 megabyte of RAM.By default, WiredTiger uses Snappy block compression for all collections and prefix compression for all indexes and we can have up to 100 open connections in the pool\nThe WiredTiger storage engine maintains lists of empty records in data files as it deletes documents. This space can be reused by WiredTiger, but will not be returned to the operating system unless under very specific circumstances.\nThe amount of empty space available for reuse by WiredTiger is reflected in the output of db.collection.stats() under the heading wiredTiger.block-manager.file bytes available for reuse.\nTo allow the WiredTiger storage engine to release this empty space to the operating system, you can de-fragment your data file. This can be achieved using the compact command. For more information on its behavior and other considerations, see compact.`` The db.serverStatus() command provides details of how much memory MongoDB is using. The following script prints out a top-level summary of memory utilization.\nlet serverStats = db.serverStatus(); print(\u0026#39;Mongod virtual memory \u0026#39;, serverStats.mem.virtual); print(\u0026#39;Mongod resident memory\u0026#39;, serverStats.mem.resident); print(\u0026#39;WiredTiger cache size\u0026#39;, ... Math.round( ... serverStats.wiredTiger.cache [\u0026#39;bytes currently in the cache\u0026#39;] / 1048576 ... ) ... ); virtual -\u0026gt; virtual memory allocated for monogdb resident -\u0026gt; Memory that used so far including cache the Wired Tiger cache will be set to either 50% of total memory minus 1GB or to 256MB The Database Cache \u0026ldquo;Hit\u0026rdquo; Ratio The database cache hit ratio is a somewhat notorious metric with a long history.Simplistically, the cache hit ratio describes how often you find a block of data you want in memory:\nCacheHitRatio = Number of IO requests that were satisfied in the cache / Total IO requests\nThe cache hit ratio represents the proportion of block requests that are satisfied by the database cache without requiring a disk read. Each “hit” – when the block is found in memory – is a good thing, since it avoids a time-consuming disk IO.\nScript to calculate\nvar cache=db.serverStatus().wiredTiger.cache; var missRatio=cache[\u0026#39;pages read into cache\u0026#39;]*100/cache[\u0026#39;pages requested from the cache\u0026#39;]; var hitRatio=100-missRatio; print(hitRatio); It will return the cache hit since server started.To calculate for specfic peroid of the time use mongo tunning mongoTuning.monitorServerDerived(5000,/cacheHitRate/)\nEvictions (putting data to disk from cache) MongoDB doesn’t wait until the cache is completely full before performing evictions.By default, MongoDB will try and keep 20% of the cache free for new data and will startto restrict new pages from coming into cache when the free percentage hits 5%\nMongoDB tries to keep the percentage of modified – “dirty” – blocks under 5%. If the percentage of modified blocks hits 20%, then operations will be blocked until the target value is achieved.\nWhen the number of clean blocks or dirty blocks hits the higher threshold values, then sessions that try to bring new blocks into the cache will be required to perform an eviction before the read operation can complete\nTo get total blocking evictions so far `db.serverStatus().wiredTiger[\u0026ldquo;thread-yield\u0026rdquo;][\u0026ldquo;page acquire eviction blocked\u0026rdquo;]\nTo get ratio\nvar wt=db.serverStatus().wiredTiger; var blockingEvictRate=wt[\u0026#39;thread-yield\u0026#39;][\u0026#39;page acquire eviction blocked\u0026#39;] *100 / wt[\u0026#39;cache\u0026#39;][\u0026#39;eviction server evicting pages\u0026#39;]; wiredTiger.cache(\u0026ldquo;application threads page read from disk to cache count\u0026rdquo;): This records the number of reads from disk into the Wired Tiger cache. application threads page read from disk to cache time (usecs): This records the number of microseconds spent moving data from disk to cache If we divide the count/time we get average time to read a page into the cache it must need between 1ms to 2ms application threads page write from cache to disk count: The number of writes from the cache to disk application threads page write from cache to disk time (usecs): The time spent writing from cache to disk Checkpoints When ever data updated the data won\u0026rsquo;t get updated in disk it will updated in cache first then only it will be updated bulk to disk to avoid I/O\nMongodb periodically ensures that the data files are synchronized with the changes in the cache. These checkpoints involve writing out the modified “dirty” blocks to disk. By default, checkpoints occur every 60 sec.\nThe eviction_dirty_trigger and eviction_dirty_target settings – control how many modified blocks are allowed in the cache before eviction processing kicks in. These can be adjusted to reduce the number of modified blocks in the cache, reducing the amount of data that must be written to disk during a checkpoint.\nDisk IO Latency and Throughput Latency describes the time it takes to retrieve a single item of information from the disk\nThroughput describes the number of IOs that can be performed by the disk devices in a given unit of time. Throughput is generally expressed in terms of IO operations per second, often abbreviated as IOPS.\nTIP: The throughput of an IO system is primarily determined by the number of physical disk devices it contains. To increase IO throughput, increase the number of physical disks in disk volumes.\nMongoDB IO MongoDB performs three major types of IO operations:\nTemporary file IO involves reads and writes to the “_tmp” directory within the dbPath directory. These IOs occur when a disk sort or disk-­based aggregation operation occurs\nDatafile IO occurs when WiredTiger reads from and writes to collection and index files in the db Path directory\nJournal file IO occurs as the WiredTiger storage engine writes to the“write-ahead” journal file.\ndb.serverStatus().wiredTiger.log log bytes written: The amount of data written to the journal. log sync operations: The number of log “sync” operations. A sync occurs when journal information held in memory is flushed to disk. log sync time duration (μsecs): The number of microseconds spent in sync operations. Replica Sets A replica set following best practice consists of a primary node together with two or more secondary nodes. It is recommended to use three or more nodes, with an odd number of total nodes. The primary node accepts all write requests which are propagated synchronously or asynchronously to the secondary nodes. In the event of a failure of a primary, an election occurs to which a secondary node is elected to serve as the new primary and database operations can continue\nRead Preference → By default, all reads are directed to the primary node. However, we can set a read preference which directs the MongoDB drivers to direct read requests to secondary nodes.\nRead Preference Effect primary This is the default. All reads are directed to the replica set primary. primaryPreferred Direct reads to the primary, but if no primary is available, direct reads to a secondary. secondary Direct reads to a secondary. secondaryPreferred Direct reads to a secondary, but if no secondary is available, direct reads to the primary. nearest Direct reads to the replica set member with the lowest network round trip time to the calling program. maxStalenessSeconds : can be added to a read preference to control the tolerable lag in data. When picking a secondary node, the MongoDB driver will only consider those nodes who have data within maxStalenessSeconds seconds of the primary. The minimum value is 90 seconds. For instance, this URL specified a preference for secondary nodes, but only if their data timestamps are within 5 minutes (300 seconds) of the primary:\nReplica Set Tag Sets: Tag sets can be used to direct read requests to specific nodes. You can use tag sets to nominate nodes for special purposes such as analytics or to distribute read workload more evenly across all nodes in the cluster.\nSharding Need to study :) ","permalink":"https://programmerraja.github.io/blog/notes/2023/mongodb-performance-tuning/","summary":"MongoDB Architecture and Concepts if the write concern is set to “majority”, then the database will not complete the write operation until a majority of secondaries receive the write. We can also set the write concern to wait until all secondaries or a specific number of secondaries receive the write operation.\nOplog The primary node stores information about document changes in a collection within its local database called the Oplog. The primary will continuously attempt to apply these changes to secondary instances.","title":"MongoDB Performance Tuning Book Notes"},{"content":"==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠==\nText Elements %%\nDrawing { \u0026#34;type\u0026#34;: \u0026#34;excalidraw\u0026#34;, \u0026#34;version\u0026#34;: 2, \u0026#34;source\u0026#34;: \u0026#34;https://github.com/zsviczian/obsidian-excalidraw-plugin/releases/tag/2.0.13\u0026#34;, \u0026#34;elements\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;lbilyLsh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;x\u0026#34;: -251, \u0026#34;y\u0026#34;: -134.0234375, \u0026#34;width\u0026#34;: 137.2798614501953, \u0026#34;height\u0026#34;: 25, \u0026#34;angle\u0026#34;: 0, \u0026#34;strokeColor\u0026#34;: \u0026#34;#1e1e1e\u0026#34;, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34;, \u0026#34;fillStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;strokeWidth\u0026#34;: 2, \u0026#34;strokeStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;roughness\u0026#34;: 1, \u0026#34;opacity\u0026#34;: 100, \u0026#34;groupIds\u0026#34;: [], \u0026#34;frameId\u0026#34;: null, \u0026#34;roundness\u0026#34;: null, \u0026#34;seed\u0026#34;: 300057372, \u0026#34;version\u0026#34;: 2, \u0026#34;versionNonce\u0026#34;: 157007772, \u0026#34;isDeleted\u0026#34;: true, \u0026#34;boundElements\u0026#34;: null, \u0026#34;updated\u0026#34;: 1705674402817, \u0026#34;link\u0026#34;: null, \u0026#34;locked\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;CTRL+Shift+V\u0026#34;, \u0026#34;rawText\u0026#34;: \u0026#34;CTRL+Shift+V\u0026#34;, \u0026#34;fontSize\u0026#34;: 20, \u0026#34;fontFamily\u0026#34;: 1, \u0026#34;textAlign\u0026#34;: \u0026#34;left\u0026#34;, \u0026#34;verticalAlign\u0026#34;: \u0026#34;top\u0026#34;, \u0026#34;baseline\u0026#34;: 18, \u0026#34;containerId\u0026#34;: null, \u0026#34;originalText\u0026#34;: \u0026#34;CTRL+Shift+V\u0026#34;, \u0026#34;lineHeight\u0026#34;: 1.25 } ], \u0026#34;appState\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;light\u0026#34;, \u0026#34;viewBackgroundColor\u0026#34;: \u0026#34;#ffffff\u0026#34;, \u0026#34;currentItemStrokeColor\u0026#34;: \u0026#34;#1e1e1e\u0026#34;, \u0026#34;currentItemBackgroundColor\u0026#34;: \u0026#34;transparent\u0026#34;, \u0026#34;currentItemFillStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;currentItemStrokeWidth\u0026#34;: 2, \u0026#34;currentItemStrokeStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;currentItemRoughness\u0026#34;: 1, \u0026#34;currentItemOpacity\u0026#34;: 100, \u0026#34;currentItemFontFamily\u0026#34;: 1, \u0026#34;currentItemFontSize\u0026#34;: 20, \u0026#34;currentItemTextAlign\u0026#34;: \u0026#34;left\u0026#34;, \u0026#34;currentItemStartArrowhead\u0026#34;: null, \u0026#34;currentItemEndArrowhead\u0026#34;: \u0026#34;arrow\u0026#34;, \u0026#34;scrollX\u0026#34;: 511, \u0026#34;scrollY\u0026#34;: 360.9765625, \u0026#34;zoom\u0026#34;: { \u0026#34;value\u0026#34;: 1 }, \u0026#34;currentItemRoundness\u0026#34;: \u0026#34;round\u0026#34;, \u0026#34;gridSize\u0026#34;: null, \u0026#34;gridColor\u0026#34;: { \u0026#34;Bold\u0026#34;: \u0026#34;#C9C9C9FF\u0026#34;, \u0026#34;Regular\u0026#34;: \u0026#34;#EDEDEDFF\u0026#34; }, \u0026#34;currentStrokeOptions\u0026#34;: null, \u0026#34;previousGridSize\u0026#34;: null, \u0026#34;frameRendering\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;clip\u0026#34;: true, \u0026#34;name\u0026#34;: true, \u0026#34;outline\u0026#34;: true } }, \u0026#34;files\u0026#34;: {} } %%\n","permalink":"https://programmerraja.github.io/blog/excalidraw/art-2024-01-19-19.56.09/","summary":"==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠==\nText Elements %%\nDrawing { \u0026#34;type\u0026#34;: \u0026#34;excalidraw\u0026#34;, \u0026#34;version\u0026#34;: 2, \u0026#34;source\u0026#34;: \u0026#34;https://github.com/zsviczian/obsidian-excalidraw-plugin/releases/tag/2.0.13\u0026#34;, \u0026#34;elements\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;lbilyLsh\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;x\u0026#34;: -251, \u0026#34;y\u0026#34;: -134.0234375, \u0026#34;width\u0026#34;: 137.2798614501953, \u0026#34;height\u0026#34;: 25, \u0026#34;angle\u0026#34;: 0, \u0026#34;strokeColor\u0026#34;: \u0026#34;#1e1e1e\u0026#34;, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34;, \u0026#34;fillStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;strokeWidth\u0026#34;: 2, \u0026#34;strokeStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;roughness\u0026#34;: 1, \u0026#34;opacity\u0026#34;: 100, \u0026#34;groupIds\u0026#34;: [], \u0026#34;frameId\u0026#34;: null, \u0026#34;roundness\u0026#34;: null, \u0026#34;seed\u0026#34;: 300057372, \u0026#34;version\u0026#34;: 2, \u0026#34;versionNonce\u0026#34;: 157007772, \u0026#34;isDeleted\u0026#34;: true, \u0026#34;boundElements\u0026#34;: null, \u0026#34;updated\u0026#34;: 1705674402817, \u0026#34;link\u0026#34;: null, \u0026#34;locked\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;CTRL+Shift+V\u0026#34;, \u0026#34;rawText\u0026#34;: \u0026#34;CTRL+Shift+V\u0026#34;, \u0026#34;fontSize\u0026#34;: 20, \u0026#34;fontFamily\u0026#34;: 1, \u0026#34;textAlign\u0026#34;: \u0026#34;left\u0026#34;, \u0026#34;verticalAlign\u0026#34;: \u0026#34;top\u0026#34;, \u0026#34;baseline\u0026#34;: 18, \u0026#34;containerId\u0026#34;: null, \u0026#34;originalText\u0026#34;: \u0026#34;CTRL+Shift+V\u0026#34;, \u0026#34;lineHeight\u0026#34;: 1.","title":""},{"content":"==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠==\nText Elements ENVOY ^OJ0WhMt5\n%%\nDrawing { \u0026#34;type\u0026#34;: \u0026#34;excalidraw\u0026#34;, \u0026#34;version\u0026#34;: 2, \u0026#34;source\u0026#34;: \u0026#34;https://github.com/zsviczian/obsidian-excalidraw-plugin/releases/tag/2.0.13\u0026#34;, \u0026#34;elements\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;bQU-nPEaqRqMW7UBezHk9\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;rectangle\u0026#34;, \u0026#34;x\u0026#34;: -187.45735692977905, \u0026#34;y\u0026#34;: -205.67471599578857, \u0026#34;width\u0026#34;: 660.553466796875, \u0026#34;height\u0026#34;: 404.21929931640625, \u0026#34;angle\u0026#34;: 0, \u0026#34;strokeColor\u0026#34;: \u0026#34;#1e1e1e\u0026#34;, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34;, \u0026#34;fillStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;strokeWidth\u0026#34;: 2, \u0026#34;strokeStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;roughness\u0026#34;: 1, \u0026#34;opacity\u0026#34;: 100, \u0026#34;groupIds\u0026#34;: [], \u0026#34;frameId\u0026#34;: null, \u0026#34;roundness\u0026#34;: { \u0026#34;type\u0026#34;: 3 }, \u0026#34;seed\u0026#34;: 1976162072, \u0026#34;version\u0026#34;: 25, \u0026#34;versionNonce\u0026#34;: 432148248, \u0026#34;isDeleted\u0026#34;: false, \u0026#34;boundElements\u0026#34;: null, \u0026#34;updated\u0026#34;: 1706173366594, \u0026#34;link\u0026#34;: null, \u0026#34;locked\u0026#34;: false }, { \u0026#34;id\u0026#34;: \u0026#34;OJ0WhMt5\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;x\u0026#34;: -118.44429540634155, \u0026#34;y\u0026#34;: -142.13888835906982, \u0026#34;width\u0026#34;: 61.539947509765625, \u0026#34;height\u0026#34;: 25, \u0026#34;angle\u0026#34;: 0, \u0026#34;strokeColor\u0026#34;: \u0026#34;#1e1e1e\u0026#34;, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34;, \u0026#34;fillStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;strokeWidth\u0026#34;: 2, \u0026#34;strokeStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;roughness\u0026#34;: 1, \u0026#34;opacity\u0026#34;: 100, \u0026#34;groupIds\u0026#34;: [], \u0026#34;frameId\u0026#34;: null, \u0026#34;roundness\u0026#34;: null, \u0026#34;seed\u0026#34;: 1507879960, \u0026#34;version\u0026#34;: 6, \u0026#34;versionNonce\u0026#34;: 919085160, \u0026#34;isDeleted\u0026#34;: false, \u0026#34;boundElements\u0026#34;: null, \u0026#34;updated\u0026#34;: 1706173373326, \u0026#34;link\u0026#34;: null, \u0026#34;locked\u0026#34;: false, \u0026#34;text\u0026#34;: \u0026#34;ENVOY\u0026#34;, \u0026#34;rawText\u0026#34;: \u0026#34;ENVOY\u0026#34;, \u0026#34;fontSize\u0026#34;: 20, \u0026#34;fontFamily\u0026#34;: 1, \u0026#34;textAlign\u0026#34;: \u0026#34;left\u0026#34;, \u0026#34;verticalAlign\u0026#34;: \u0026#34;top\u0026#34;, \u0026#34;baseline\u0026#34;: 16, \u0026#34;containerId\u0026#34;: null, \u0026#34;originalText\u0026#34;: \u0026#34;ENVOY\u0026#34;, \u0026#34;lineHeight\u0026#34;: 1.25 } ], \u0026#34;appState\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;light\u0026#34;, \u0026#34;viewBackgroundColor\u0026#34;: \u0026#34;#ffffff\u0026#34;, \u0026#34;currentItemStrokeColor\u0026#34;: \u0026#34;#1e1e1e\u0026#34;, \u0026#34;currentItemBackgroundColor\u0026#34;: \u0026#34;transparent\u0026#34;, \u0026#34;currentItemFillStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;currentItemStrokeWidth\u0026#34;: 2, \u0026#34;currentItemStrokeStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;currentItemRoughness\u0026#34;: 1, \u0026#34;currentItemOpacity\u0026#34;: 100, \u0026#34;currentItemFontFamily\u0026#34;: 1, \u0026#34;currentItemFontSize\u0026#34;: 20, \u0026#34;currentItemTextAlign\u0026#34;: \u0026#34;left\u0026#34;, \u0026#34;currentItemStartArrowhead\u0026#34;: null, \u0026#34;currentItemEndArrowhead\u0026#34;: \u0026#34;arrow\u0026#34;, \u0026#34;scrollX\u0026#34;: 760.4607443809509, \u0026#34;scrollY\u0026#34;: 426.1159315109253, \u0026#34;zoom\u0026#34;: { \u0026#34;value\u0026#34;: 1 }, \u0026#34;currentItemRoundness\u0026#34;: \u0026#34;round\u0026#34;, \u0026#34;gridSize\u0026#34;: null, \u0026#34;gridColor\u0026#34;: { \u0026#34;Bold\u0026#34;: \u0026#34;#C9C9C9FF\u0026#34;, \u0026#34;Regular\u0026#34;: \u0026#34;#EDEDEDFF\u0026#34; }, \u0026#34;currentStrokeOptions\u0026#34;: null, \u0026#34;previousGridSize\u0026#34;: null, \u0026#34;frameRendering\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;clip\u0026#34;: true, \u0026#34;name\u0026#34;: true, \u0026#34;outline\u0026#34;: true } }, \u0026#34;files\u0026#34;: {} } %%\n","permalink":"https://programmerraja.github.io/blog/excalidraw/art-2024-01-25-14.32.40/","summary":"==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠==\nText Elements ENVOY ^OJ0WhMt5\n%%\nDrawing { \u0026#34;type\u0026#34;: \u0026#34;excalidraw\u0026#34;, \u0026#34;version\u0026#34;: 2, \u0026#34;source\u0026#34;: \u0026#34;https://github.com/zsviczian/obsidian-excalidraw-plugin/releases/tag/2.0.13\u0026#34;, \u0026#34;elements\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;bQU-nPEaqRqMW7UBezHk9\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;rectangle\u0026#34;, \u0026#34;x\u0026#34;: -187.45735692977905, \u0026#34;y\u0026#34;: -205.67471599578857, \u0026#34;width\u0026#34;: 660.553466796875, \u0026#34;height\u0026#34;: 404.21929931640625, \u0026#34;angle\u0026#34;: 0, \u0026#34;strokeColor\u0026#34;: \u0026#34;#1e1e1e\u0026#34;, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34;, \u0026#34;fillStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;strokeWidth\u0026#34;: 2, \u0026#34;strokeStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;roughness\u0026#34;: 1, \u0026#34;opacity\u0026#34;: 100, \u0026#34;groupIds\u0026#34;: [], \u0026#34;frameId\u0026#34;: null, \u0026#34;roundness\u0026#34;: { \u0026#34;type\u0026#34;: 3 }, \u0026#34;seed\u0026#34;: 1976162072, \u0026#34;version\u0026#34;: 25, \u0026#34;versionNonce\u0026#34;: 432148248, \u0026#34;isDeleted\u0026#34;: false, \u0026#34;boundElements\u0026#34;: null, \u0026#34;updated\u0026#34;: 1706173366594, \u0026#34;link\u0026#34;: null, \u0026#34;locked\u0026#34;: false }, { \u0026#34;id\u0026#34;: \u0026#34;OJ0WhMt5\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;x\u0026#34;: -118.","title":""},{"content":"==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠==\n%%\nDrawing {\u0026#34;type\u0026#34;:\u0026#34;excalidraw\u0026#34;,\u0026#34;version\u0026#34;:2,\u0026#34;source\u0026#34;:\u0026#34;https://github.com/zsviczian/obsidian-excalidraw-plugin/releases/tag/2.0.13\u0026#34;,\u0026#34;elements\u0026#34;:[],\u0026#34;appState\u0026#34;:{\u0026#34;gridSize\u0026#34;:null,\u0026#34;viewBackgroundColor\u0026#34;:\u0026#34;#ffffff\u0026#34;}} %%\n","permalink":"https://programmerraja.github.io/blog/excalidraw/art-2024-02-11-07.27.35/","summary":"==⚠ Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠==\n%%\nDrawing {\u0026#34;type\u0026#34;:\u0026#34;excalidraw\u0026#34;,\u0026#34;version\u0026#34;:2,\u0026#34;source\u0026#34;:\u0026#34;https://github.com/zsviczian/obsidian-excalidraw-plugin/releases/tag/2.0.13\u0026#34;,\u0026#34;elements\u0026#34;:[],\u0026#34;appState\u0026#34;:{\u0026#34;gridSize\u0026#34;:null,\u0026#34;viewBackgroundColor\u0026#34;:\u0026#34;#ffffff\u0026#34;}} %%","title":""}]